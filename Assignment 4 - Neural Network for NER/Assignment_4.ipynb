{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4\n",
        "Anushka Deshpande \\\n",
        "USC ID: 5914892345"
      ],
      "metadata": {
        "id": "ALjSAgTknZHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions:\n",
        "\n",
        "Task 1: \n",
        "1. What are the precision, recall and F1 scores on dev data?\\\n",
        "precision:  78.63%; recall:  72.50%; F-1 score:  75.44\n",
        "\n",
        "\n",
        "Task 2:\n",
        "1. How to deal with the conflict of capitalisaation? \\\n",
        "After trying multiple ways of dealing with capital words, including boolean mask, it was observed that ignoring the case of the word returns the best trained model. Hence, for this submission , the case of the word has been ignored.\n",
        "\n",
        "2. What are the precision, recall and F1 scores on dev data?\\\n",
        "precision:  88.27%; recall:  89.38%; FB1:  88.82"
      ],
      "metadata": {
        "id": "bxDY9wO9zxgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n",
        "\n",
        "Here we are importing the required libraries into the program. "
      ],
      "metadata": {
        "id": "mY8e78QInipa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "febcYdWgDSXl",
        "outputId": "f88d1bc7-2757-4f25-f688-9ea1811ca846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrnCKdiID928"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/HW4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lChSPThDAJrp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
        "import random\n",
        "import json\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are setting the device to cuda if available to make use of maximum computation power available to us."
      ],
      "metadata": {
        "id": "bdF3XnWontV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoiStz3jAJrs"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a class, createData. It contains the following functions detailed below:\n",
        "\n",
        "1. SplitTrainXY: \\\n",
        "This function splits the train data into x_train and y_train. This function is also called to split validation data into x_val and y_val.\n",
        "\n",
        "2. readFileTrain: \\\n",
        "This function reads the train data from the file, calls the SplitTrainXY function to split the data and returns x_train and y_train. It is also used to obtain x_val and y_val from validation data.\n",
        "\n",
        "3. splitTestX: \\\n",
        "This function gives the test data in the required format. It matches the data format of x_test with x_train. \n",
        "\n",
        "4. readFileTest: \\\n",
        "This funciton reads the test data from the data file and calls splitTestX on the test data, and returns x_test.\n"
      ],
      "metadata": {
        "id": "6RbjNqign24P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkGxbmZ7AJrt",
        "outputId": "bcb3ba8d-4d97-4015-cb47-b4fe2a38ea83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14987 14987\n",
            "3466 3466\n",
            "3684\n"
          ]
        }
      ],
      "source": [
        "class createData():\n",
        "    \n",
        "  def SplitTrainXY(self, dataset):\n",
        "      x_train, y_train = list(), list()\n",
        "      x, y = list(), list()\n",
        "      first = 1\n",
        "      for entry in dataset.itertuples():\n",
        "          # print(type(entry.id))\n",
        "          # break\n",
        "          if(entry.ID == '1' and first == 0):\n",
        "              x_train.append(x)\n",
        "              y_train.append(y)\n",
        "              x = list()\n",
        "              y = list()\n",
        "          first = 0\n",
        "          x.append(entry.Word)\n",
        "          y.append(entry.NERTag)\n",
        "\n",
        "      x_train.append(x)\n",
        "      y_train.append(y)\n",
        "\n",
        "      return x_train, y_train\n",
        "\n",
        "  def readFileTrain(self, filepath):\n",
        "      train_df = list()\n",
        "      with open(filepath, 'r') as inputFile:\n",
        "          for line in inputFile.readlines():\n",
        "              if len(line) > 2:\n",
        "                id, word, ner_tag = line.strip().split(\" \")\n",
        "                train_df.append([id, word, ner_tag])\n",
        "\n",
        "      train_df = pd.DataFrame(train_df, columns=['ID', 'Word', 'NERTag'])\n",
        "      train_df = train_df.dropna()\n",
        "      x_train, y_train = self.SplitTrainXY(train_df)\n",
        "      return x_train, y_train\n",
        "\n",
        "  def SplitTestX(self, dataset):\n",
        "      x_train = list()\n",
        "      x = list()\n",
        "      first = 1\n",
        "      for entry in dataset.itertuples():\n",
        "          # print(type(entry.id))\n",
        "          # break\n",
        "          if(entry.ID == '1' and first == 0):\n",
        "              x_train.append(x)\n",
        "              x = list()\n",
        "          first = 0\n",
        "          x.append(entry.Word)\n",
        "\n",
        "      x_train.append(x)\n",
        "      return x_train\n",
        "\n",
        "  def readFileTest(self, filepath):\n",
        "      train_df = list()\n",
        "      with open(filepath, 'r') as inputFile:\n",
        "          for line in inputFile.readlines():\n",
        "              if len(line) > 1:\n",
        "                  id, word = line.strip().split(\" \")\n",
        "                  train_df.append([id, word])\n",
        "\n",
        "      train_df = pd.DataFrame(train_df, columns=['ID', 'Word'])\n",
        "      train_df = train_df.dropna()\n",
        "      x_train = self.SplitTestX(train_df)\n",
        "      return x_train\n",
        "\n",
        "obj = createData()\n",
        "x_train, y_train = obj.readFileTrain('./data/train')\n",
        "x_val, y_val = obj.readFileTrain('./data/dev')\n",
        "x_test = obj.readFileTest('./data/test')\n",
        "\n",
        "print(len(x_train), len(y_train))\n",
        "print(len(x_val), len(y_val))\n",
        "print(len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a class getVectors() which is used to generate and return the various vectors and matrices required for training the BiLSTM models. The variou functions are described as follows:\n",
        "\n",
        "1. prepareVocabulary(): \\\n",
        "This function prepares a list of unique words in the entire dataset, and returns it as a set. \n",
        "\n",
        "2. prepareLabelDictionary(): \\\n",
        "This function prepares label tags for each word from each sentence in the training data. The output of this function is a matrix of labels for each word in each sentence in the dataset.\n",
        "\n",
        "3. prepareWordIndex(): \\\n",
        "This function prepares a dictionary of all the words available and gives them an index. This is stored in the form of a dictionary of the form a word-index pair.\n",
        "\n",
        "4. vectorizeSentence(): \\\n",
        "This function is used to convert the train data from word wise, to sentence wise format. Each entry in the returned data is the vectors for each word in one sentence on the data.\n",
        "\n",
        "5. vectorizeLabel(): \\\n",
        "This function prepares label tags for each word in each sentence.\n",
        "\n",
        "6. createEmbedMatrix(): \\\n",
        "This function creates and returns the embedding matrix for Glove data. \n",
        "\n",
        "7. initialiseClassWeights(): \\\n",
        "This function initialises and returns the class weights to be assigned to the model during initialisation. "
      ],
      "metadata": {
        "id": "T7v3gKRepB4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUeilvDqAJru"
      },
      "outputs": [],
      "source": [
        "class getVectors():\n",
        "\n",
        "  def prepareVocabulary(self, dataset):\n",
        "      vocab = set()\n",
        "      for sentence in dataset:\n",
        "          for word in sentence:\n",
        "              vocab.add(word)\n",
        "      return vocab\n",
        "  \n",
        "  def prepareLabelDictionary(self, train_y, val_y):\n",
        "      label1 = self.prepareVocabulary(train_y)\n",
        "      label2 = self.prepareVocabulary(val_y)\n",
        "      label = label1.union(label2)\n",
        "      label_tuples = []\n",
        "      counter = 0\n",
        "      for tags in label:\n",
        "          label_tuples.append((tags, counter))\n",
        "          counter += 1\n",
        "      label_dict = dict(label_tuples)\n",
        "      return label_dict\n",
        "\n",
        "  def prepareWordIndex(self, train, val):\n",
        "      wordIdx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "      idx = 2\n",
        "      for data in [train, val]:\n",
        "          for sent in data:\n",
        "              for word in sent:\n",
        "                  if word not in wordIdx:\n",
        "                      wordIdx[word] = idx\n",
        "                      idx += 1\n",
        "      return wordIdx\n",
        "\n",
        "  def vectorizeSentence(self, train, wordIdx):\n",
        "      train_vec = list()\n",
        "      tmp_x = list()\n",
        "      for words in train:\n",
        "          for word in words:\n",
        "            if word not in wordIdx.keys():\n",
        "              tmp_x.append(wordIdx['<UNK>'])\n",
        "            else:\n",
        "              tmp_x.append(wordIdx[word])\n",
        "          train_vec.append(tmp_x)\n",
        "          tmp_x = list()\n",
        "      return train_vec\n",
        "\n",
        "  def vectorizeLabel(self, train_y, label_dict):\n",
        "      train_y_vec = list()\n",
        "      for tags in train_y:\n",
        "          tmp_yy = list()\n",
        "          for label in tags:\n",
        "              tmp_yy.append(label_dict[label])\n",
        "          train_y_vec.append(tmp_yy)\n",
        "      return train_y_vec\n",
        "  \n",
        "  def createEmbedMatrix(self, wordIdx, embedDictionary, dimension):\n",
        "      embedMatrix = np.zeros((len(wordIdx), dimension))\n",
        "      for word, idx in wordIdx.items():\n",
        "          if word in embedDictionary:\n",
        "              embedMatrix[idx] = embedDictionary[word]\n",
        "          else:\n",
        "              if word.lower() in embedDictionary:\n",
        "                  embedMatrix[idx] = embedDictionary[word.lower()] + 5e-3\n",
        "              else:\n",
        "                  embedMatrix[idx] = embedDictionary[\"<UNK>\"]\n",
        "\n",
        "      return embedMatrix\n",
        "      \n",
        "  def initializeClassWeights(self, label_dict, train_y, val_y):\n",
        "    classWeights = dict()\n",
        "    for key in label_dict:\n",
        "        classWeights[key] = 0\n",
        "    total_nm_tags = 0\n",
        "    for data in [train_y, val_y]:\n",
        "        for tags in data:\n",
        "            for tag in tags:\n",
        "                total_nm_tags += 1\n",
        "                classWeights[tag] += 1\n",
        "\n",
        "    classWt = list()\n",
        "    for key in classWeights.keys():\n",
        "        if classWeights[key]:\n",
        "            score = round(math.log(0.35*total_nm_tags / classWeights[key]), 2)\n",
        "            classWeights[key] = score if score > 1.0 else 1.0\n",
        "        else:\n",
        "            classWeights[key] = 1.0\n",
        "        classWt.append(classWeights[key])\n",
        "    classWt = torch.tensor(classWt)\n",
        "    return classWt\n",
        "\n",
        "\n",
        "obj2 = getVectors()\n",
        "word_idx = obj2.prepareWordIndex(x_train, x_val)\n",
        "x_train_vectors = obj2.vectorizeSentence(x_train, word_idx)\n",
        "x_test_vectors = obj2.vectorizeSentence(x_test, word_idx)\n",
        "x_val_vectors = obj2.vectorizeSentence(x_val, word_idx)\n",
        "label_dictionary = obj2.prepareLabelDictionary(y_train, y_val)\n",
        "y_train_vectors = obj2.vectorizeLabel(y_train, label_dictionary)\n",
        "y_val_vectors = obj2.vectorizeLabel(y_val, label_dictionary)\n",
        "\n",
        "class_wt = obj2.initializeClassWeights(label_dictionary, y_train, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Simple Bidirectional LSTM\n",
        "\n",
        "The class below creates a BiLSTM model with the following parameters and architecture:\n",
        "\n",
        "Architecture: Embedding layer --> LSTM --> Linear layer --> ELU --> Classifier\n",
        "\n",
        "Parameters: \\\n",
        "1. Embedding Dimension: 100\n",
        "2. Number of LSTM layers: 1\n",
        "3. LSTM Hidden Dimensions: 256\n",
        "4. LSTM Dropout: 0.33\n",
        "5. Linear Layer Output Dimensions: 128"
      ],
      "metadata": {
        "id": "Qbuc86f1ubSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTbgOXdqNqUk"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDimensions, linearOutputDimension, hiddenLayerDimension, LSTMLayers,\n",
        "                 bidirectional, dropoutValue, tagSize):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        \"\"\" Hyper Parameters \"\"\"\n",
        "        self.hiddenLayerDimension = hiddenLayerDimension \n",
        "        self.LSTMLayers = LSTMLayers \n",
        "        self.embedDimensions = embedDimensions \n",
        "        self.linearOutputDimension = linearOutputDimension  \n",
        "        self.tagSize = tagSize \n",
        "        if bidirectional:\n",
        "          self.num_directions = 2 \n",
        "        else:\n",
        "          self.num_directions = 1\n",
        "\n",
        "        \"\"\" Initializing Network \"\"\"\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocabSize, embedDimensions) \n",
        "        self.embedding.weight.data.uniform_(-1,1)\n",
        "\n",
        "        #BiLSTM Layer\n",
        "        self.LSTM = nn.LSTM(embedDimensions,\n",
        "                            hiddenLayerDimension,\n",
        "                            num_layers=LSTMLayers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        #Linear Layer\n",
        "        self.fc = nn.Linear(hiddenLayerDimension*self.num_directions,\n",
        "                            linearOutputDimension)  \n",
        "        \n",
        "        #Dropout Layer\n",
        "        self.dropout = nn.Dropout(dropoutValue)\n",
        "\n",
        "        #ELU Layer\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "\n",
        "        #Classifier \n",
        "        self.classifier = nn.Linear(linearOutputDimension, self.tagSize)\n",
        "\n",
        "    def init_hidden(self, batchSize):\n",
        "        h, c = (torch.zeros(self.LSTMLayers * self.num_directions,\n",
        "                            batchSize, self.hiddenLayerDimension).to(device),\n",
        "                torch.zeros(self.LSTMLayers * self.num_directions,\n",
        "                            batchSize, self.hiddenLayerDimension).to(device))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, sentence, sentenceLength):  \n",
        "        # Set initial states\n",
        "        batchSize = sentence.shape[0]\n",
        "        h_0, c_0 = self.init_hidden(batchSize)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        embedded = self.embedding(sentence).float()\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded, sentenceLength, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
        "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        dropout = self.dropout(output_unpacked)\n",
        "        linear = self.fc(dropout)\n",
        "        prediction = self.elu(linear)\n",
        "        prediction = self.classifier(prediction)\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Glove word Embeddings\n",
        "\n",
        "The class below creates a BiLSTM model with the glove vectors with the following parameters and architecture: \n",
        "\n",
        "Architecture: Embedding layer --> LSTM --> Linear layer --> ELU --> Classifier\n",
        "\n",
        "Parameters: \\\n",
        "1. Embedding Dimension: 100\n",
        "2. Number of LSTM layers: 1\n",
        "3. LSTM Hidden Dimensions: 256\n",
        "4. LSTM Dropout: 0.33\n",
        "5. Linear Layer Output Dimensions: 128"
      ],
      "metadata": {
        "id": "0e2O5E7ywQQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D25mPaFSNtkW"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_Glove(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDimension, linearOutputDimension, hiddenLayerDimension, LSTMLayers,\n",
        "                 bidirectional, dropoutValue, tagSize, embedMatrix):\n",
        "        super(BiLSTM_Glove, self).__init__()\n",
        "        \"\"\" Hyper Parameters \"\"\"\n",
        "        self.hiddenLayerDimension = hiddenLayerDimension \n",
        "        self.LSTMLayers = LSTMLayers \n",
        "        self.embedDimension = embedDimension \n",
        "        self.linearOutputDimension = linearOutputDimension \n",
        "        self.tagSize = tagSize \n",
        "        self.embedMatrix = embedMatrix\n",
        "        if bidirectional:\n",
        "          self.numDirections = 2 \n",
        "        else:\n",
        "          self.numDirections = 1\n",
        "\n",
        "        \"\"\" Initializing Network \"\"\"\n",
        "        #Embedding layer\n",
        "        self.embedding = nn.Embedding(vocabSize, embedDimension)  \n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedMatrix))\n",
        "\n",
        "        #LSTM Layer\n",
        "        self.LSTM = nn.LSTM(embedDimension,\n",
        "                            hiddenLayerDimension,\n",
        "                            num_layers=LSTMLayers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        #Linear layer\n",
        "        self.fc = nn.Linear(hiddenLayerDimension*self.numDirections, linearOutputDimension) \n",
        "\n",
        "        #Dropout layer\n",
        "        self.dropout = nn.Dropout(dropoutValue)\n",
        "\n",
        "        #ELU Layer\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "\n",
        "        #CLassifier\n",
        "        self.classifier = nn.Linear(linearOutputDimension, self.tagSize)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h, c = (torch.zeros(self.LSTMLayers * self.numDirections,\n",
        "                            batch_size, self.hiddenLayerDimension).to(device),\n",
        "                torch.zeros(self.LSTMLayers * self.numDirections,\n",
        "                            batch_size, self.hiddenLayerDimension).to(device))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, sentence, sentenceLength):  \n",
        "        #Set initial states\n",
        "        batch_size = sentence.shape[0]\n",
        "        h_0, c_0 = self.init_hidden(batch_size)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        embedded = self.embedding(sentence).float()\n",
        "        packed_embedded = pack_padded_sequence(embedded, sentenceLength, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
        "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        dropout = self.dropout(output_unpacked)\n",
        "        linear = self.fc(dropout)\n",
        "        prediction = self.elu(linear)\n",
        "        prediction = self.classifier(prediction)\n",
        "        return prediction\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two classes below, BiLSTM_DataLoader and BiLSTM_TestLoader load the dataset by converting the data into torch tensors. "
      ],
      "metadata": {
        "id": "9t3FeWCawizR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmSIKPMINwUr"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_DataLoader(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        xInstance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
        "        yInstance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
        "        return xInstance, yInstance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAcsN_l6NyLZ"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_TestLoader(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        xInstance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
        "        return xInstance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two classes below, CustomCollator and CustomTestCollator both implement padding for the input data by considering the max padding length as the max length in the Batch. "
      ],
      "metadata": {
        "id": "hRGut-k4wwNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN20kC25N0jm"
      },
      "outputs": [],
      "source": [
        "class CustomCollator(object):\n",
        "\n",
        "    def __init__(self, vocabulary, label):\n",
        "        self.params = vocabulary\n",
        "        self.label = label\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        (xx, yy) = zip(*batch)\n",
        "        xLength = [len(x) for x in xx]\n",
        "        ylength = [len(y) for y in yy]\n",
        "        maxBatchlength = max([len(s) for s in xx])\n",
        "        batchData = self.params['<PAD>']*np.ones((len(xx), maxBatchlength))\n",
        "        batchLabels = -1*np.zeros((len(xx), maxBatchlength))\n",
        "        for j in range(len(xx)):\n",
        "            cur_len = len(xx[j])\n",
        "            batchData[j][:cur_len] = xx[j]\n",
        "            batchLabels[j][:cur_len] = yy[j]\n",
        "\n",
        "        batchData, batchLabels = torch.LongTensor(\n",
        "            batchData), torch.LongTensor(batchLabels)\n",
        "        batchData, batchLabels = Variable(batchData), Variable(batchLabels)\n",
        "\n",
        "        return batchData, batchLabels, xLength, ylength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nb9cE1wOuUi"
      },
      "outputs": [],
      "source": [
        "class CustomTestCollator(object):\n",
        "\n",
        "    def __init__(self, vocabulary, label):\n",
        "        self.params = vocabulary\n",
        "        self.label = label\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        xx = batch\n",
        "        xLen = [len(x) for x in xx]\n",
        "        maxBatchlength = max([len(s) for s in xx])\n",
        "        batchData = self.params['<PAD>']*np.ones((len(xx), maxBatchlength))\n",
        "        for j in range(len(xx)):\n",
        "            cur_len = len(xx[j])\n",
        "            batchData[j][:cur_len] = xx[j]\n",
        "\n",
        "        batchData = torch.LongTensor(batchData)\n",
        "        batchData = Variable(batchData)\n",
        "\n",
        "        return batchData, xLen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training:\n",
        "\n",
        "The class below is used for training the data. It has 2 functions, train_lstm, which trains the Simple BiLSTM model, and train_glove which trains the Glove model using the training and dev datasets. \n",
        "\n",
        "1. Simple BiLSTM Training: \\\n",
        "It is trained using the following parameters: \\\n",
        "  *   batch size: 4\n",
        "  *   optimizer: SGD, with learning_rate = 0.1 and momentum = 0.9\n",
        "  *   scheduler: StepLR, with step_size = 20 and gamma = 0.9\n",
        "\n",
        "  This model is trained for 50 epochs and we achieve  \n",
        "    * accuracy : 95.00\n",
        "    * validation F1 score : 74.92\n",
        "\n",
        "\n",
        "\n",
        "2. BiLSTM Training using Glove Embeddings: \\\n",
        "It is trained using the following parameters: \\\n",
        "  *   batch size: 8\n",
        "  *   optimizer: SGD, with learning_rate = 0.1 and momentum = 0.9\n",
        "  *   scheduler: StepLR, with step_size = 15 and gamma = 0.9\n",
        "\n",
        "  This model is trained for 25 epochs and we achieve  \n",
        "    * accuracy : 97.95\n",
        "    * validation F1 score : 88.82\n",
        "\n",
        "The dev outputs are stored in 2 files, dev1.out and dev2.out.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dbGG424ZxGRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y00TP8QbAJrx",
        "outputId": "a3150352-f63a-4005-f3d4-1839978de528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiLSTM(\n",
            "  (embedding): Embedding(26886, 100)\n",
            "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (dropout): Dropout(p=0.33, inplace=False)\n",
            "  (elu): ELU(alpha=0.01)\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "Epoch: 1 \tTraining Loss: 2.286941\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 9137 phrases; correct: 1717.\n",
            "accuracy:  76.16%; precision:  18.79%; recall:  28.90%; FB1:  22.77\n",
            "              LOC: precision:  35.17%; recall:  43.17%; FB1:  38.76  2255\n",
            "             MISC: precision:   4.90%; recall:   0.54%; FB1:   0.98  102\n",
            "              ORG: precision:  90.00%; recall:   0.67%; FB1:   1.33  10\n",
            "              PER: precision:  13.44%; recall:  49.40%; FB1:  21.13  6770\n",
            "Epoch: 2 \tTraining Loss: 1.600799\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 7499 phrases; correct: 2556.\n",
            "accuracy:  83.93%; precision:  34.08%; recall:  43.02%; FB1:  38.03\n",
            "              LOC: precision:  55.10%; recall:  52.97%; FB1:  54.01  1766\n",
            "             MISC: precision:  38.40%; recall:  26.57%; FB1:  31.41  638\n",
            "              ORG: precision:  49.72%; recall:  20.21%; FB1:  28.74  545\n",
            "              PER: precision:  23.45%; recall:  57.93%; FB1:  33.39  4550\n",
            "Epoch: 3 \tTraining Loss: 1.190215\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 8702 phrases; correct: 3237.\n",
            "accuracy:  85.87%; precision:  37.20%; recall:  54.48%; FB1:  44.21\n",
            "              LOC: precision:  57.31%; recall:  63.80%; FB1:  60.38  2045\n",
            "             MISC: precision:  51.43%; recall:  44.79%; FB1:  47.88  803\n",
            "              ORG: precision:  24.68%; recall:  42.80%; FB1:  31.31  2326\n",
            "              PER: precision:  30.56%; recall:  58.52%; FB1:  40.15  3528\n",
            "Epoch: 4 \tTraining Loss: 0.897996\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6973 phrases; correct: 3533.\n",
            "accuracy:  90.00%; precision:  50.67%; recall:  59.46%; FB1:  54.71\n",
            "              LOC: precision:  70.13%; recall:  67.99%; FB1:  69.04  1781\n",
            "             MISC: precision:  56.92%; recall:  60.63%; FB1:  58.72  982\n",
            "              ORG: precision:  41.08%; recall:  46.01%; FB1:  43.40  1502\n",
            "              PER: precision:  40.92%; recall:  60.15%; FB1:  48.70  2708\n",
            "Epoch: 5 \tTraining Loss: 0.696627\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 7397 phrases; correct: 3761.\n",
            "accuracy:  90.15%; precision:  50.84%; recall:  63.30%; FB1:  56.39\n",
            "              LOC: precision:  73.90%; recall:  67.50%; FB1:  70.55  1678\n",
            "             MISC: precision:  61.12%; recall:  61.71%; FB1:  61.41  931\n",
            "              ORG: precision:  39.06%; recall:  56.82%; FB1:  46.29  1951\n",
            "              PER: precision:  41.95%; recall:  64.60%; FB1:  50.87  2837\n",
            "Epoch: 6 \tTraining Loss: 0.530556\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 7524 phrases; correct: 3911.\n",
            "accuracy:  90.62%; precision:  51.98%; recall:  65.82%; FB1:  58.09\n",
            "              LOC: precision:  77.32%; recall:  71.26%; FB1:  74.16  1693\n",
            "             MISC: precision:  56.62%; recall:  65.84%; FB1:  60.88  1072\n",
            "              ORG: precision:  37.90%; recall:  61.22%; FB1:  46.82  2166\n",
            "              PER: precision:  45.28%; recall:  63.74%; FB1:  52.94  2593\n",
            "Epoch: 7 \tTraining Loss: 0.415911\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 7366 phrases; correct: 3998.\n",
            "accuracy:  91.10%; precision:  54.28%; recall:  67.28%; FB1:  60.08\n",
            "              LOC: precision:  78.81%; recall:  72.07%; FB1:  75.29  1680\n",
            "             MISC: precision:  58.99%; recall:  66.16%; FB1:  62.37  1034\n",
            "              ORG: precision:  40.19%; recall:  63.09%; FB1:  49.10  2105\n",
            "              PER: precision:  47.82%; recall:  66.12%; FB1:  55.50  2547\n",
            "Epoch: 8 \tTraining Loss: 0.341073\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6937 phrases; correct: 3993.\n",
            "accuracy:  91.97%; precision:  57.56%; recall:  67.20%; FB1:  62.01\n",
            "              LOC: precision:  82.84%; recall:  72.02%; FB1:  77.05  1597\n",
            "             MISC: precision:  47.56%; recall:  69.74%; FB1:  56.55  1352\n",
            "              ORG: precision:  43.52%; recall:  62.86%; FB1:  51.43  1937\n",
            "              PER: precision:  57.73%; recall:  64.28%; FB1:  60.83  2051\n",
            "Epoch: 9 \tTraining Loss: 0.258078\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6798 phrases; correct: 4105.\n",
            "accuracy:  92.55%; precision:  60.39%; recall:  69.08%; FB1:  64.44\n",
            "              LOC: precision:  79.42%; recall:  75.61%; FB1:  77.47  1749\n",
            "             MISC: precision:  59.77%; recall:  68.98%; FB1:  64.05  1064\n",
            "              ORG: precision:  42.81%; recall:  66.82%; FB1:  52.18  2093\n",
            "              PER: precision:  62.58%; recall:  64.28%; FB1:  63.42  1892\n",
            "Epoch: 10 \tTraining Loss: 0.217166\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6538 phrases; correct: 4060.\n",
            "accuracy:  92.75%; precision:  62.10%; recall:  68.33%; FB1:  65.06\n",
            "              LOC: precision:  84.13%; recall:  72.46%; FB1:  77.86  1582\n",
            "             MISC: precision:  57.16%; recall:  71.91%; FB1:  63.69  1160\n",
            "              ORG: precision:  47.03%; recall:  63.76%; FB1:  54.13  1818\n",
            "              PER: precision:  61.22%; recall:  65.74%; FB1:  63.40  1978\n",
            "Epoch: 11 \tTraining Loss: 0.176500\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6123 phrases; correct: 4111.\n",
            "accuracy:  93.59%; precision:  67.14%; recall:  69.19%; FB1:  68.15\n",
            "              LOC: precision:  81.50%; recall:  75.56%; FB1:  78.42  1703\n",
            "             MISC: precision:  66.43%; recall:  72.56%; FB1:  69.36  1007\n",
            "              ORG: precision:  52.28%; recall:  65.03%; FB1:  57.96  1668\n",
            "              PER: precision:  67.74%; recall:  64.17%; FB1:  65.90  1745\n",
            "Epoch: 12 \tTraining Loss: 0.148061\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6078 phrases; correct: 4133.\n",
            "accuracy:  93.68%; precision:  68.00%; recall:  69.56%; FB1:  68.77\n",
            "              LOC: precision:  82.67%; recall:  76.10%; FB1:  79.25  1691\n",
            "             MISC: precision:  64.29%; recall:  72.45%; FB1:  68.13  1039\n",
            "              ORG: precision:  54.91%; recall:  65.92%; FB1:  59.91  1610\n",
            "              PER: precision:  68.07%; recall:  64.22%; FB1:  66.09  1738\n",
            "Epoch: 13 \tTraining Loss: 0.123249\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6073 phrases; correct: 4154.\n",
            "accuracy:  93.81%; precision:  68.40%; recall:  69.91%; FB1:  69.15\n",
            "              LOC: precision:  84.35%; recall:  76.27%; FB1:  80.10  1661\n",
            "             MISC: precision:  63.72%; recall:  72.78%; FB1:  67.95  1053\n",
            "              ORG: precision:  56.64%; recall:  66.14%; FB1:  61.03  1566\n",
            "              PER: precision:  66.65%; recall:  64.88%; FB1:  65.75  1793\n",
            "Epoch: 14 \tTraining Loss: 0.152857\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 7026 phrases; correct: 4162.\n",
            "accuracy:  92.56%; precision:  59.24%; recall:  70.04%; FB1:  64.19\n",
            "              LOC: precision:  77.81%; recall:  74.63%; FB1:  76.19  1762\n",
            "             MISC: precision:  51.08%; recall:  72.13%; FB1:  59.80  1302\n",
            "              ORG: precision:  47.31%; recall:  66.89%; FB1:  55.42  1896\n",
            "              PER: precision:  59.49%; recall:  66.72%; FB1:  62.90  2066\n",
            "Epoch: 15 \tTraining Loss: 0.121703\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5697 phrases; correct: 4070.\n",
            "accuracy:  94.03%; precision:  71.44%; recall:  68.50%; FB1:  69.94\n",
            "              LOC: precision:  82.36%; recall:  77.03%; FB1:  79.61  1718\n",
            "             MISC: precision:  71.91%; recall:  71.37%; FB1:  71.64  915\n",
            "              ORG: precision:  58.37%; recall:  66.07%; FB1:  61.98  1518\n",
            "              PER: precision:  71.86%; recall:  60.31%; FB1:  65.58  1546\n",
            "Epoch: 16 \tTraining Loss: 0.103677\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 6267 phrases; correct: 4114.\n",
            "accuracy:  93.44%; precision:  65.65%; recall:  69.24%; FB1:  67.39\n",
            "              LOC: precision:  73.55%; recall:  78.88%; FB1:  76.12  1970\n",
            "             MISC: precision:  57.47%; recall:  71.80%; FB1:  63.84  1152\n",
            "              ORG: precision:  56.76%; recall:  65.10%; FB1:  60.65  1538\n",
            "              PER: precision:  70.32%; recall:  61.35%; FB1:  65.53  1607\n",
            "Epoch: 17 \tTraining Loss: 0.082174\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5479 phrases; correct: 4055.\n",
            "accuracy:  94.18%; precision:  74.01%; recall:  68.24%; FB1:  71.01\n",
            "              LOC: precision:  83.14%; recall:  78.12%; FB1:  80.55  1726\n",
            "             MISC: precision:  72.25%; recall:  71.15%; FB1:  71.69  908\n",
            "              ORG: precision:  64.63%; recall:  63.09%; FB1:  63.85  1309\n",
            "              PER: precision:  72.79%; recall:  60.69%; FB1:  66.19  1536\n",
            "Epoch: 18 \tTraining Loss: 0.067518\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5544 phrases; correct: 4087.\n",
            "accuracy:  94.28%; precision:  73.72%; recall:  68.78%; FB1:  71.16\n",
            "              LOC: precision:  82.72%; recall:  78.72%; FB1:  80.67  1748\n",
            "             MISC: precision:  74.34%; recall:  72.89%; FB1:  73.60  904\n",
            "              ORG: precision:  62.46%; recall:  66.14%; FB1:  64.25  1420\n",
            "              PER: precision:  73.51%; recall:  58.74%; FB1:  65.30  1472\n",
            "Epoch: 19 \tTraining Loss: 0.056590\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5468 phrases; correct: 4097.\n",
            "accuracy:  94.38%; precision:  74.93%; recall:  68.95%; FB1:  71.81\n",
            "              LOC: precision:  87.15%; recall:  74.58%; FB1:  80.38  1572\n",
            "             MISC: precision:  74.72%; recall:  73.10%; FB1:  73.90  902\n",
            "              ORG: precision:  62.36%; recall:  66.96%; FB1:  64.58  1440\n",
            "              PER: precision:  74.32%; recall:  62.70%; FB1:  68.02  1554\n",
            "Epoch: 20 \tTraining Loss: 0.058175\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5532 phrases; correct: 4150.\n",
            "accuracy:  94.45%; precision:  75.02%; recall:  69.84%; FB1:  72.34\n",
            "              LOC: precision:  84.96%; recall:  76.86%; FB1:  80.71  1662\n",
            "             MISC: precision:  75.62%; recall:  73.32%; FB1:  74.45  894\n",
            "              ORG: precision:  62.98%; recall:  67.11%; FB1:  64.98  1429\n",
            "              PER: precision:  75.11%; recall:  63.08%; FB1:  68.57  1547\n",
            "Epoch: 21 \tTraining Loss: 0.044728\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5595 phrases; correct: 4118.\n",
            "accuracy:  94.28%; precision:  73.60%; recall:  69.30%; FB1:  71.39\n",
            "              LOC: precision:  84.62%; recall:  76.70%; FB1:  80.47  1665\n",
            "             MISC: precision:  68.87%; recall:  73.43%; FB1:  71.08  983\n",
            "              ORG: precision:  64.07%; recall:  68.08%; FB1:  66.02  1425\n",
            "              PER: precision:  73.52%; recall:  60.75%; FB1:  66.53  1522\n",
            "Epoch: 22 \tTraining Loss: 0.039586\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5572 phrases; correct: 4192.\n",
            "accuracy:  94.64%; precision:  75.23%; recall:  70.55%; FB1:  72.82\n",
            "              LOC: precision:  84.92%; recall:  77.57%; FB1:  81.08  1678\n",
            "             MISC: precision:  76.15%; recall:  75.16%; FB1:  75.66  910\n",
            "              ORG: precision:  65.48%; recall:  67.49%; FB1:  66.47  1382\n",
            "              PER: precision:  72.97%; recall:  63.46%; FB1:  67.89  1602\n",
            "Epoch: 23 \tTraining Loss: 0.035595\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5469 phrases; correct: 4171.\n",
            "accuracy:  94.60%; precision:  76.27%; recall:  70.20%; FB1:  73.10\n",
            "              LOC: precision:  83.19%; recall:  78.93%; FB1:  81.01  1743\n",
            "             MISC: precision:  76.44%; recall:  74.95%; FB1:  75.68  904\n",
            "              ORG: precision:  67.86%; recall:  66.29%; FB1:  67.07  1310\n",
            "              PER: precision:  75.46%; recall:  61.94%; FB1:  68.04  1512\n",
            "Epoch: 24 \tTraining Loss: 0.028848\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5454 phrases; correct: 4161.\n",
            "accuracy:  94.58%; precision:  76.29%; recall:  70.03%; FB1:  73.03\n",
            "              LOC: precision:  84.88%; recall:  78.82%; FB1:  81.74  1706\n",
            "             MISC: precision:  77.87%; recall:  73.64%; FB1:  75.70  872\n",
            "              ORG: precision:  65.23%; recall:  67.56%; FB1:  66.37  1389\n",
            "              PER: precision:  75.86%; recall:  61.24%; FB1:  67.77  1487\n",
            "Epoch: 25 \tTraining Loss: 0.021718\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5554 phrases; correct: 4188.\n",
            "accuracy:  94.70%; precision:  75.41%; recall:  70.48%; FB1:  72.86\n",
            "              LOC: precision:  85.40%; recall:  78.33%; FB1:  81.71  1685\n",
            "             MISC: precision:  72.62%; recall:  73.64%; FB1:  73.13  935\n",
            "              ORG: precision:  66.00%; recall:  66.89%; FB1:  66.44  1359\n",
            "              PER: precision:  74.48%; recall:  63.68%; FB1:  68.66  1575\n",
            "Epoch: 26 \tTraining Loss: 0.021043\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5536 phrases; correct: 4194.\n",
            "accuracy:  94.65%; precision:  75.76%; recall:  70.58%; FB1:  73.08\n",
            "              LOC: precision:  84.65%; recall:  79.53%; FB1:  82.01  1726\n",
            "             MISC: precision:  77.07%; recall:  73.64%; FB1:  75.32  881\n",
            "              ORG: precision:  65.78%; recall:  67.93%; FB1:  66.84  1385\n",
            "              PER: precision:  74.03%; recall:  62.05%; FB1:  67.51  1544\n",
            "Epoch: 27 \tTraining Loss: 0.021660\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5583 phrases; correct: 4268.\n",
            "accuracy:  94.79%; precision:  76.45%; recall:  71.83%; FB1:  74.07\n",
            "              LOC: precision:  83.98%; recall:  79.31%; FB1:  81.58  1735\n",
            "             MISC: precision:  77.79%; recall:  73.32%; FB1:  75.49  869\n",
            "              ORG: precision:  68.04%; recall:  69.20%; FB1:  68.61  1364\n",
            "              PER: precision:  74.74%; recall:  65.53%; FB1:  69.83  1615\n",
            "Epoch: 28 \tTraining Loss: 0.019925\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5525 phrases; correct: 4219.\n",
            "accuracy:  94.76%; precision:  76.36%; recall:  71.00%; FB1:  73.59\n",
            "              LOC: precision:  84.94%; recall:  78.93%; FB1:  81.83  1707\n",
            "             MISC: precision:  78.82%; recall:  75.05%; FB1:  76.89  878\n",
            "              ORG: precision:  66.76%; recall:  68.46%; FB1:  67.60  1375\n",
            "              PER: precision:  74.06%; recall:  62.92%; FB1:  68.04  1565\n",
            "Epoch: 29 \tTraining Loss: 0.016874\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5492 phrases; correct: 4192.\n",
            "accuracy:  94.73%; precision:  76.33%; recall:  70.55%; FB1:  73.33\n",
            "              LOC: precision:  83.00%; recall:  80.24%; FB1:  81.59  1776\n",
            "             MISC: precision:  82.09%; recall:  72.56%; FB1:  77.03  815\n",
            "              ORG: precision:  67.26%; recall:  67.26%; FB1:  67.26  1341\n",
            "              PER: precision:  73.53%; recall:  62.27%; FB1:  67.43  1560\n",
            "Epoch: 30 \tTraining Loss: 0.017910\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5440 phrases; correct: 4213.\n",
            "accuracy:  94.74%; precision:  77.44%; recall:  70.90%; FB1:  74.03\n",
            "              LOC: precision:  84.23%; recall:  79.37%; FB1:  81.73  1731\n",
            "             MISC: precision:  78.86%; recall:  73.21%; FB1:  75.93  856\n",
            "              ORG: precision:  71.45%; recall:  67.19%; FB1:  69.25  1261\n",
            "              PER: precision:  74.06%; recall:  64.01%; FB1:  68.67  1592\n",
            "Epoch: 31 \tTraining Loss: 0.019566\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5408 phrases; correct: 4184.\n",
            "accuracy:  94.78%; precision:  77.37%; recall:  70.41%; FB1:  73.73\n",
            "              LOC: precision:  84.72%; recall:  78.17%; FB1:  81.31  1695\n",
            "             MISC: precision:  78.34%; recall:  74.51%; FB1:  76.38  877\n",
            "              ORG: precision:  69.82%; recall:  66.07%; FB1:  67.89  1269\n",
            "              PER: precision:  74.98%; recall:  63.79%; FB1:  68.94  1567\n",
            "Epoch: 32 \tTraining Loss: 0.018810\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5501 phrases; correct: 4214.\n",
            "accuracy:  94.77%; precision:  76.60%; recall:  70.92%; FB1:  73.65\n",
            "              LOC: precision:  85.36%; recall:  79.37%; FB1:  82.26  1708\n",
            "             MISC: precision:  79.28%; recall:  73.86%; FB1:  76.47  859\n",
            "              ORG: precision:  66.81%; recall:  67.86%; FB1:  67.33  1362\n",
            "              PER: precision:  74.11%; recall:  63.25%; FB1:  68.25  1572\n",
            "Epoch: 33 \tTraining Loss: 0.016230\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5431 phrases; correct: 4196.\n",
            "accuracy:  94.69%; precision:  77.26%; recall:  70.62%; FB1:  73.79\n",
            "              LOC: precision:  84.35%; recall:  79.21%; FB1:  81.70  1725\n",
            "             MISC: precision:  80.05%; recall:  74.40%; FB1:  77.12  857\n",
            "              ORG: precision:  66.91%; recall:  68.75%; FB1:  67.82  1378\n",
            "              PER: precision:  77.02%; recall:  61.51%; FB1:  68.40  1471\n",
            "Epoch: 34 \tTraining Loss: 0.019808\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5400 phrases; correct: 4189.\n",
            "accuracy:  94.76%; precision:  77.57%; recall:  70.50%; FB1:  73.87\n",
            "              LOC: precision:  86.13%; recall:  78.77%; FB1:  82.29  1680\n",
            "             MISC: precision:  79.58%; recall:  73.54%; FB1:  76.44  852\n",
            "              ORG: precision:  68.34%; recall:  68.08%; FB1:  68.21  1336\n",
            "              PER: precision:  75.13%; recall:  62.49%; FB1:  68.23  1532\n",
            "Epoch: 35 \tTraining Loss: 0.015742\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5433 phrases; correct: 4231.\n",
            "accuracy:  94.89%; precision:  77.88%; recall:  71.20%; FB1:  74.39\n",
            "              LOC: precision:  85.93%; recall:  78.82%; FB1:  82.23  1685\n",
            "             MISC: precision:  81.26%; recall:  74.30%; FB1:  77.62  843\n",
            "              ORG: precision:  67.73%; recall:  68.23%; FB1:  67.98  1351\n",
            "              PER: precision:  76.13%; recall:  64.22%; FB1:  69.67  1554\n",
            "Epoch: 36 \tTraining Loss: 0.014252\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5547 phrases; correct: 4256.\n",
            "accuracy:  94.86%; precision:  76.73%; recall:  71.63%; FB1:  74.09\n",
            "              LOC: precision:  86.14%; recall:  78.50%; FB1:  82.14  1674\n",
            "             MISC: precision:  78.60%; recall:  75.27%; FB1:  76.90  883\n",
            "              ORG: precision:  68.60%; recall:  68.08%; FB1:  68.34  1331\n",
            "              PER: precision:  72.75%; recall:  65.53%; FB1:  68.95  1659\n",
            "Epoch: 37 \tTraining Loss: 0.015681\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5432 phrases; correct: 4205.\n",
            "accuracy:  94.81%; precision:  77.41%; recall:  70.77%; FB1:  73.94\n",
            "              LOC: precision:  85.94%; recall:  79.21%; FB1:  82.44  1693\n",
            "             MISC: precision:  77.45%; recall:  74.51%; FB1:  75.95  887\n",
            "              ORG: precision:  70.10%; recall:  66.96%; FB1:  68.50  1281\n",
            "              PER: precision:  74.16%; recall:  63.25%; FB1:  68.27  1571\n",
            "Epoch: 38 \tTraining Loss: 0.015527\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5524 phrases; correct: 4232.\n",
            "accuracy:  94.76%; precision:  76.61%; recall:  71.22%; FB1:  73.82\n",
            "              LOC: precision:  86.45%; recall:  79.21%; FB1:  82.67  1683\n",
            "             MISC: precision:  77.74%; recall:  74.62%; FB1:  76.15  885\n",
            "              ORG: precision:  66.28%; recall:  69.05%; FB1:  67.64  1397\n",
            "              PER: precision:  74.60%; recall:  63.14%; FB1:  68.39  1559\n",
            "Epoch: 39 \tTraining Loss: 0.013861\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5581 phrases; correct: 4293.\n",
            "accuracy:  94.92%; precision:  76.92%; recall:  72.25%; FB1:  74.51\n",
            "              LOC: precision:  85.80%; recall:  79.59%; FB1:  82.58  1704\n",
            "             MISC: precision:  78.84%; recall:  73.97%; FB1:  76.33  865\n",
            "              ORG: precision:  67.20%; recall:  68.75%; FB1:  67.97  1372\n",
            "              PER: precision:  74.82%; recall:  66.61%; FB1:  70.48  1640\n",
            "Epoch: 40 \tTraining Loss: 0.011278\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5586 phrases; correct: 4272.\n",
            "accuracy:  94.86%; precision:  76.48%; recall:  71.89%; FB1:  74.12\n",
            "              LOC: precision:  86.30%; recall:  78.55%; FB1:  82.25  1672\n",
            "             MISC: precision:  78.73%; recall:  75.05%; FB1:  76.85  879\n",
            "              ORG: precision:  66.52%; recall:  68.61%; FB1:  67.55  1383\n",
            "              PER: precision:  73.67%; recall:  66.07%; FB1:  69.66  1652\n",
            "Epoch: 41 \tTraining Loss: 0.009079\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5489 phrases; correct: 4277.\n",
            "accuracy:  94.91%; precision:  77.92%; recall:  71.98%; FB1:  74.83\n",
            "              LOC: precision:  84.91%; recall:  81.16%; FB1:  82.99  1756\n",
            "             MISC: precision:  81.49%; recall:  74.51%; FB1:  77.85  843\n",
            "              ORG: precision:  68.16%; recall:  68.16%; FB1:  68.16  1341\n",
            "              PER: precision:  76.50%; recall:  64.33%; FB1:  69.89  1549\n",
            "Epoch: 42 \tTraining Loss: 0.008741\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5604 phrases; correct: 4330.\n",
            "accuracy:  94.92%; precision:  77.27%; recall:  72.87%; FB1:  75.00\n",
            "              LOC: precision:  86.16%; recall:  79.31%; FB1:  82.60  1691\n",
            "             MISC: precision:  76.78%; recall:  76.03%; FB1:  76.40  913\n",
            "              ORG: precision:  69.05%; recall:  69.05%; FB1:  69.05  1341\n",
            "              PER: precision:  75.11%; recall:  67.64%; FB1:  71.18  1659\n",
            "Epoch: 43 \tTraining Loss: 0.009012\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5587 phrases; correct: 4292.\n",
            "accuracy:  94.91%; precision:  76.82%; recall:  72.23%; FB1:  74.46\n",
            "              LOC: precision:  84.49%; recall:  80.08%; FB1:  82.22  1741\n",
            "             MISC: precision:  83.35%; recall:  74.95%; FB1:  78.93  829\n",
            "              ORG: precision:  67.23%; recall:  68.23%; FB1:  67.73  1361\n",
            "              PER: precision:  73.37%; recall:  65.96%; FB1:  69.47  1656\n",
            "Epoch: 44 \tTraining Loss: 0.007351\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5573 phrases; correct: 4266.\n",
            "accuracy:  94.83%; precision:  76.55%; recall:  71.79%; FB1:  74.09\n",
            "              LOC: precision:  85.87%; recall:  79.42%; FB1:  82.52  1699\n",
            "             MISC: precision:  78.79%; recall:  73.75%; FB1:  76.19  863\n",
            "              ORG: precision:  67.38%; recall:  68.38%; FB1:  67.88  1361\n",
            "              PER: precision:  73.33%; recall:  65.69%; FB1:  69.30  1650\n",
            "Epoch: 45 \tTraining Loss: 0.009088\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5545 phrases; correct: 4306.\n",
            "accuracy:  94.96%; precision:  77.66%; recall:  72.47%; FB1:  74.97\n",
            "              LOC: precision:  86.96%; recall:  78.44%; FB1:  82.48  1657\n",
            "             MISC: precision:  77.96%; recall:  75.60%; FB1:  76.76  894\n",
            "              ORG: precision:  67.24%; recall:  69.95%; FB1:  68.57  1395\n",
            "              PER: precision:  76.92%; recall:  66.78%; FB1:  71.49  1599\n",
            "Epoch: 46 \tTraining Loss: 0.006375\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5568 phrases; correct: 4306.\n",
            "accuracy:  95.01%; precision:  77.33%; recall:  72.47%; FB1:  74.82\n",
            "              LOC: precision:  86.40%; recall:  78.88%; FB1:  82.47  1677\n",
            "             MISC: precision:  79.88%; recall:  74.51%; FB1:  77.10  860\n",
            "              ORG: precision:  67.40%; recall:  68.61%; FB1:  68.00  1365\n",
            "              PER: precision:  75.03%; recall:  67.86%; FB1:  71.27  1666\n",
            "Epoch: 47 \tTraining Loss: 0.006515\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5522 phrases; correct: 4308.\n",
            "accuracy:  95.01%; precision:  78.02%; recall:  72.50%; FB1:  75.16\n",
            "              LOC: precision:  86.41%; recall:  79.97%; FB1:  83.06  1700\n",
            "             MISC: precision:  79.56%; recall:  75.16%; FB1:  77.30  871\n",
            "              ORG: precision:  69.39%; recall:  69.65%; FB1:  69.52  1346\n",
            "              PER: precision:  75.51%; recall:  65.80%; FB1:  70.32  1605\n",
            "Epoch: 48 \tTraining Loss: 0.007109\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5479 phrases; correct: 4308.\n",
            "accuracy:  95.03%; precision:  78.63%; recall:  72.50%; FB1:  75.44\n",
            "              LOC: precision:  87.10%; recall:  79.37%; FB1:  83.05  1674\n",
            "             MISC: precision:  80.18%; recall:  76.36%; FB1:  78.22  878\n",
            "              ORG: precision:  71.01%; recall:  68.68%; FB1:  69.83  1297\n",
            "              PER: precision:  75.15%; recall:  66.50%; FB1:  70.56  1630\n",
            "Epoch: 49 \tTraining Loss: 0.005872\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "processed 51578 tokens with 5942 phrases; found: 5569 phrases; correct: 4317.\n",
            "accuracy:  95.00%; precision:  77.52%; recall:  72.65%; FB1:  75.01\n",
            "              LOC: precision:  86.10%; recall:  80.62%; FB1:  83.27  1720\n",
            "             MISC: precision:  80.39%; recall:  75.60%; FB1:  77.92  867\n",
            "              ORG: precision:  67.13%; recall:  68.98%; FB1:  68.04  1378\n",
            "              PER: precision:  75.69%; recall:  65.91%; FB1:  70.46  1604\n",
            "Epoch: 50 \tTraining Loss: 0.006487\n",
            "{'I-MISC': 0, 'B-PER': 1, 'I-LOC': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-PER': 7, 'B-MISC': 8}\n",
            "model saved\n",
            "processed 51578 tokens with 5942 phrases; found: 5543 phrases; correct: 4302.\n",
            "accuracy:  95.00%; precision:  77.61%; recall:  72.40%; FB1:  74.92\n",
            "              LOC: precision:  82.44%; recall:  80.51%; FB1:  81.47  1794\n",
            "             MISC: precision:  82.27%; recall:  73.97%; FB1:  77.90  829\n",
            "              ORG: precision:  70.62%; recall:  69.20%; FB1:  69.91  1314\n",
            "              PER: precision:  75.53%; recall:  65.85%; FB1:  70.36  1606\n",
            "26886 100\n",
            "BiLSTM_Glove(\n",
            "  (embedding): Embedding(26886, 100)\n",
            "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (dropout): Dropout(p=0.33, inplace=False)\n",
            "  (elu): ELU(alpha=0.01)\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "Epoch: 1 \tTraining Loss: 0.635067\n",
            "processed 51578 tokens with 5942 phrases; found: 7678 phrases; correct: 4438.\n",
            "accuracy:  92.19%; precision:  57.80%; recall:  74.69%; FB1:  65.17\n",
            "              LOC: precision:  71.42%; recall:  82.69%; FB1:  76.64  2127\n",
            "             MISC: precision:  37.22%; recall:  59.54%; FB1:  45.81  1475\n",
            "              ORG: precision:  42.53%; recall:  62.64%; FB1:  50.66  1975\n",
            "              PER: precision:  72.82%; recall:  83.06%; FB1:  77.61  2101\n",
            "Epoch: 2 \tTraining Loss: 0.297861\n",
            "processed 51578 tokens with 5942 phrases; found: 6504 phrases; correct: 4735.\n",
            "accuracy:  95.28%; precision:  72.80%; recall:  79.69%; FB1:  76.09\n",
            "              LOC: precision:  79.15%; recall:  86.39%; FB1:  82.61  2005\n",
            "             MISC: precision:  56.43%; recall:  68.98%; FB1:  62.08  1127\n",
            "              ORG: precision:  61.22%; recall:  68.16%; FB1:  64.50  1493\n",
            "              PER: precision:  85.05%; recall:  86.75%; FB1:  85.89  1879\n",
            "Epoch: 3 \tTraining Loss: 0.199908\n",
            "processed 51578 tokens with 5942 phrases; found: 6282 phrases; correct: 4944.\n",
            "accuracy:  96.33%; precision:  78.70%; recall:  83.20%; FB1:  80.89\n",
            "              LOC: precision:  84.27%; recall:  88.68%; FB1:  86.42  1933\n",
            "             MISC: precision:  70.35%; recall:  71.04%; FB1:  70.70  931\n",
            "              ORG: precision:  64.24%; recall:  74.87%; FB1:  69.15  1563\n",
            "              PER: precision:  89.27%; recall:  89.90%; FB1:  89.59  1855\n",
            "Epoch: 4 \tTraining Loss: 0.154381\n",
            "processed 51578 tokens with 5942 phrases; found: 6377 phrases; correct: 5014.\n",
            "accuracy:  96.49%; precision:  78.63%; recall:  84.38%; FB1:  81.40\n",
            "              LOC: precision:  84.38%; recall:  90.58%; FB1:  87.37  1972\n",
            "             MISC: precision:  65.73%; recall:  73.86%; FB1:  69.56  1036\n",
            "              ORG: precision:  66.53%; recall:  75.32%; FB1:  70.65  1518\n",
            "              PER: precision:  89.63%; recall:  90.07%; FB1:  89.85  1851\n",
            "Epoch: 5 \tTraining Loss: 0.124990\n",
            "processed 51578 tokens with 5942 phrases; found: 6384 phrases; correct: 5096.\n",
            "accuracy:  96.70%; precision:  79.82%; recall:  85.76%; FB1:  82.69\n",
            "              LOC: precision:  85.87%; recall:  90.64%; FB1:  88.19  1939\n",
            "             MISC: precision:  71.52%; recall:  74.08%; FB1:  72.78  955\n",
            "              ORG: precision:  65.29%; recall:  78.97%; FB1:  71.48  1622\n",
            "              PER: precision:  90.42%; recall:  91.69%; FB1:  91.05  1868\n",
            "Epoch: 6 \tTraining Loss: 0.104237\n",
            "processed 51578 tokens with 5942 phrases; found: 6337 phrases; correct: 5138.\n",
            "accuracy:  96.92%; precision:  81.08%; recall:  86.47%; FB1:  83.69\n",
            "              LOC: precision:  86.57%; recall:  92.32%; FB1:  89.36  1959\n",
            "             MISC: precision:  68.67%; recall:  76.79%; FB1:  72.50  1031\n",
            "              ORG: precision:  69.93%; recall:  78.75%; FB1:  74.08  1510\n",
            "              PER: precision:  91.34%; recall:  91.10%; FB1:  91.22  1837\n",
            "Epoch: 7 \tTraining Loss: 0.086890\n",
            "processed 51578 tokens with 5942 phrases; found: 6291 phrases; correct: 5193.\n",
            "accuracy:  97.17%; precision:  82.55%; recall:  87.39%; FB1:  84.90\n",
            "              LOC: precision:  88.30%; recall:  92.00%; FB1:  90.11  1914\n",
            "             MISC: precision:  70.90%; recall:  78.74%; FB1:  74.61  1024\n",
            "              ORG: precision:  72.33%; recall:  80.91%; FB1:  76.38  1500\n",
            "              PER: precision:  91.31%; recall:  91.86%; FB1:  91.58  1853\n",
            "Epoch: 8 \tTraining Loss: 0.074044\n",
            "processed 51578 tokens with 5942 phrases; found: 6251 phrases; correct: 5201.\n",
            "accuracy:  97.27%; precision:  83.20%; recall:  87.53%; FB1:  85.31\n",
            "              LOC: precision:  88.35%; recall:  92.49%; FB1:  90.37  1923\n",
            "             MISC: precision:  71.75%; recall:  79.07%; FB1:  75.23  1016\n",
            "              ORG: precision:  73.41%; recall:  80.91%; FB1:  76.98  1478\n",
            "              PER: precision:  92.04%; recall:  91.64%; FB1:  91.84  1834\n",
            "Epoch: 9 \tTraining Loss: 0.062817\n",
            "processed 51578 tokens with 5942 phrases; found: 6205 phrases; correct: 5231.\n",
            "accuracy:  97.44%; precision:  84.30%; recall:  88.03%; FB1:  86.13\n",
            "              LOC: precision:  89.10%; recall:  93.41%; FB1:  91.20  1926\n",
            "             MISC: precision:  74.20%; recall:  80.80%; FB1:  77.36  1004\n",
            "              ORG: precision:  75.47%; recall:  81.66%; FB1:  78.44  1451\n",
            "              PER: precision:  91.83%; recall:  90.93%; FB1:  91.38  1824\n",
            "Epoch: 10 \tTraining Loss: 0.052564\n",
            "processed 51578 tokens with 5942 phrases; found: 6208 phrases; correct: 5215.\n",
            "accuracy:  97.40%; precision:  84.00%; recall:  87.77%; FB1:  85.84\n",
            "              LOC: precision:  89.32%; recall:  93.36%; FB1:  91.30  1920\n",
            "             MISC: precision:  69.99%; recall:  80.69%; FB1:  74.96  1063\n",
            "              ORG: precision:  76.75%; recall:  80.98%; FB1:  78.81  1415\n",
            "              PER: precision:  92.27%; recall:  90.66%; FB1:  91.46  1810\n",
            "Epoch: 11 \tTraining Loss: 0.045550\n",
            "processed 51578 tokens with 5942 phrases; found: 6261 phrases; correct: 5237.\n",
            "accuracy:  97.38%; precision:  83.64%; recall:  88.14%; FB1:  85.83\n",
            "              LOC: precision:  89.52%; recall:  93.47%; FB1:  91.45  1918\n",
            "             MISC: precision:  70.60%; recall:  81.78%; FB1:  75.78  1068\n",
            "              ORG: precision:  75.43%; recall:  81.95%; FB1:  78.56  1457\n",
            "              PER: precision:  91.69%; recall:  90.50%; FB1:  91.09  1818\n",
            "Epoch: 12 \tTraining Loss: 0.038955\n",
            "processed 51578 tokens with 5942 phrases; found: 6202 phrases; correct: 5242.\n",
            "accuracy:  97.57%; precision:  84.52%; recall:  88.22%; FB1:  86.33\n",
            "              LOC: precision:  91.08%; recall:  93.36%; FB1:  92.20  1883\n",
            "             MISC: precision:  73.12%; recall:  81.13%; FB1:  76.92  1023\n",
            "              ORG: precision:  74.10%; recall:  81.51%; FB1:  77.63  1475\n",
            "              PER: precision:  92.59%; recall:  91.53%; FB1:  92.06  1821\n",
            "Epoch: 13 \tTraining Loss: 0.034013\n",
            "processed 51578 tokens with 5942 phrases; found: 6132 phrases; correct: 5285.\n",
            "accuracy:  97.67%; precision:  86.19%; recall:  88.94%; FB1:  87.54\n",
            "              LOC: precision:  91.86%; recall:  93.41%; FB1:  92.63  1868\n",
            "             MISC: precision:  75.63%; recall:  81.13%; FB1:  78.28  989\n",
            "              ORG: precision:  77.34%; recall:  83.74%; FB1:  80.42  1452\n",
            "              PER: precision:  93.14%; recall:  92.18%; FB1:  92.66  1823\n",
            "Epoch: 14 \tTraining Loss: 0.029575\n",
            "processed 51578 tokens with 5942 phrases; found: 6224 phrases; correct: 5295.\n",
            "accuracy:  97.65%; precision:  85.07%; recall:  89.11%; FB1:  87.05\n",
            "              LOC: precision:  91.49%; recall:  93.63%; FB1:  92.55  1880\n",
            "             MISC: precision:  74.80%; recall:  82.10%; FB1:  78.28  1012\n",
            "              ORG: precision:  75.75%; recall:  83.37%; FB1:  79.38  1476\n",
            "              PER: precision:  91.59%; recall:  92.29%; FB1:  91.94  1856\n",
            "Epoch: 15 \tTraining Loss: 0.024607\n",
            "processed 51578 tokens with 5942 phrases; found: 6128 phrases; correct: 5263.\n",
            "accuracy:  97.68%; precision:  85.88%; recall:  88.57%; FB1:  87.21\n",
            "              LOC: precision:  91.58%; recall:  93.58%; FB1:  92.57  1877\n",
            "             MISC: precision:  74.65%; recall:  81.45%; FB1:  77.90  1006\n",
            "              ORG: precision:  77.51%; recall:  82.48%; FB1:  79.91  1427\n",
            "              PER: precision:  92.79%; recall:  91.59%; FB1:  92.19  1818\n",
            "Epoch: 16 \tTraining Loss: 0.022043\n",
            "processed 51578 tokens with 5942 phrases; found: 6164 phrases; correct: 5268.\n",
            "accuracy:  97.66%; precision:  85.46%; recall:  88.66%; FB1:  87.03\n",
            "              LOC: precision:  92.63%; recall:  92.38%; FB1:  92.50  1832\n",
            "             MISC: precision:  75.12%; recall:  81.56%; FB1:  78.21  1001\n",
            "              ORG: precision:  76.40%; recall:  83.52%; FB1:  79.80  1466\n",
            "              PER: precision:  91.10%; recall:  92.24%; FB1:  91.66  1865\n",
            "Epoch: 17 \tTraining Loss: 0.020534\n",
            "processed 51578 tokens with 5942 phrases; found: 6260 phrases; correct: 5311.\n",
            "accuracy:  97.65%; precision:  84.84%; recall:  89.38%; FB1:  87.05\n",
            "              LOC: precision:  92.43%; recall:  93.09%; FB1:  92.76  1850\n",
            "             MISC: precision:  72.23%; recall:  82.65%; FB1:  77.09  1055\n",
            "              ORG: precision:  76.42%; recall:  84.12%; FB1:  80.09  1476\n",
            "              PER: precision:  91.06%; recall:  92.89%; FB1:  91.96  1879\n",
            "Epoch: 18 \tTraining Loss: 0.017094\n",
            "processed 51578 tokens with 5942 phrases; found: 6073 phrases; correct: 5286.\n",
            "accuracy:  97.79%; precision:  87.04%; recall:  88.96%; FB1:  87.99\n",
            "              LOC: precision:  92.54%; recall:  93.85%; FB1:  93.19  1863\n",
            "             MISC: precision:  79.75%; recall:  81.56%; FB1:  80.64  943\n",
            "              ORG: precision:  78.62%; recall:  83.89%; FB1:  81.17  1431\n",
            "              PER: precision:  91.78%; recall:  91.48%; FB1:  91.63  1836\n",
            "Epoch: 19 \tTraining Loss: 0.014965\n",
            "processed 51578 tokens with 5942 phrases; found: 6067 phrases; correct: 5305.\n",
            "accuracy:  97.88%; precision:  87.44%; recall:  89.28%; FB1:  88.35\n",
            "              LOC: precision:  92.44%; recall:  93.79%; FB1:  93.11  1864\n",
            "             MISC: precision:  81.00%; recall:  80.91%; FB1:  80.95  921\n",
            "              ORG: precision:  78.92%; recall:  84.04%; FB1:  81.40  1428\n",
            "              PER: precision:  92.18%; recall:  92.78%; FB1:  92.48  1854\n",
            "Epoch: 20 \tTraining Loss: 0.013844\n",
            "processed 51578 tokens with 5942 phrases; found: 6116 phrases; correct: 5307.\n",
            "accuracy:  97.83%; precision:  86.77%; recall:  89.31%; FB1:  88.02\n",
            "              LOC: precision:  91.79%; recall:  93.74%; FB1:  92.76  1876\n",
            "             MISC: precision:  80.36%; recall:  82.10%; FB1:  81.22  942\n",
            "              ORG: precision:  77.78%; recall:  84.04%; FB1:  80.79  1449\n",
            "              PER: precision:  92.00%; recall:  92.35%; FB1:  92.17  1849\n",
            "Epoch: 21 \tTraining Loss: 0.012208\n",
            "processed 51578 tokens with 5942 phrases; found: 5997 phrases; correct: 5278.\n",
            "accuracy:  97.85%; precision:  88.01%; recall:  88.83%; FB1:  88.42\n",
            "              LOC: precision:  92.89%; recall:  93.20%; FB1:  93.04  1843\n",
            "             MISC: precision:  80.95%; recall:  81.56%; FB1:  81.25  929\n",
            "              ORG: precision:  79.50%; recall:  83.00%; FB1:  81.21  1400\n",
            "              PER: precision:  93.21%; recall:  92.35%; FB1:  92.77  1825\n",
            "Epoch: 22 \tTraining Loss: 0.011121\n",
            "processed 51578 tokens with 5942 phrases; found: 6075 phrases; correct: 5298.\n",
            "accuracy:  97.85%; precision:  87.21%; recall:  89.16%; FB1:  88.18\n",
            "              LOC: precision:  91.62%; recall:  93.47%; FB1:  92.54  1874\n",
            "             MISC: precision:  81.26%; recall:  81.34%; FB1:  81.30  923\n",
            "              ORG: precision:  79.03%; recall:  83.45%; FB1:  81.18  1416\n",
            "              PER: precision:  91.94%; recall:  92.94%; FB1:  92.44  1862\n",
            "Epoch: 23 \tTraining Loss: 0.009576\n",
            "processed 51578 tokens with 5942 phrases; found: 6003 phrases; correct: 5298.\n",
            "accuracy:  97.90%; precision:  88.26%; recall:  89.16%; FB1:  88.71\n",
            "              LOC: precision:  93.02%; recall:  93.58%; FB1:  93.30  1848\n",
            "             MISC: precision:  81.28%; recall:  81.45%; FB1:  81.37  924\n",
            "              ORG: precision:  80.82%; recall:  83.30%; FB1:  82.04  1382\n",
            "              PER: precision:  92.54%; recall:  92.89%; FB1:  92.71  1849\n",
            "Epoch: 24 \tTraining Loss: 0.009073\n",
            "processed 51578 tokens with 5942 phrases; found: 6030 phrases; correct: 5316.\n",
            "accuracy:  97.91%; precision:  88.16%; recall:  89.46%; FB1:  88.81\n",
            "              LOC: precision:  92.20%; recall:  94.61%; FB1:  93.39  1885\n",
            "             MISC: precision:  81.58%; recall:  81.67%; FB1:  81.63  923\n",
            "              ORG: precision:  81.78%; recall:  83.67%; FB1:  82.71  1372\n",
            "              PER: precision:  92.05%; recall:  92.45%; FB1:  92.25  1850\n",
            "Epoch: 25 \tTraining Loss: 0.007688\n",
            "model saved\n",
            "processed 51578 tokens with 5942 phrases; found: 6017 phrases; correct: 5311.\n",
            "accuracy:  97.95%; precision:  88.27%; recall:  89.38%; FB1:  88.82\n",
            "              LOC: precision:  91.84%; recall:  94.34%; FB1:  93.07  1887\n",
            "             MISC: precision:  83.56%; recall:  81.56%; FB1:  82.55  900\n",
            "              ORG: precision:  80.82%; recall:  83.89%; FB1:  82.33  1392\n",
            "              PER: precision:  92.55%; recall:  92.35%; FB1:  92.45  1838\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.train at 0x7f465baf9e80>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, MultiStepLR, ReduceLROnPlateau\n",
        "\n",
        "class train():\n",
        "\n",
        "  def train_lstm(self):\n",
        "    BiLSTM_model = BiLSTM(vocabSize=len(word_idx),\n",
        "                          embedDimensions=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary))\n",
        "    BiLSTM_model.to(device)\n",
        "    print(BiLSTM_model)\n",
        "\n",
        "    BiLSTM_train = BiLSTM_DataLoader(x_train_vectors, y_train_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader = DataLoader(dataset=BiLSTM_train,\n",
        "                            batch_size=4,\n",
        "                            drop_last=True,\n",
        "                            collate_fn=custom_collator)\n",
        "\n",
        "    BiLSTM_dev = BiLSTM_DataLoader(x_val_vectors, y_val_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
        "                                batch_size=1,\n",
        "                                shuffle=False,\n",
        "                                drop_last=True,\n",
        "                                collate_fn=custom_collator)\n",
        "\n",
        "    criteria = nn.CrossEntropyLoss(weight=class_wt)\n",
        "    criteria = criteria.to(device)\n",
        "    criteria.requres_grad = True\n",
        "    optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
        "    scheduler = StepLR(optimizer, step_size = 20, gamma=0.9)\n",
        "    epochs = 50\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = 0.0\n",
        "        for input, label, inputLength, labelLength in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = BiLSTM_model(input.to(device), inputLength)\n",
        "            output = output.view(-1, len(label_dictionary))\n",
        "            label = label.view(-1)\n",
        "            loss = criteria(output, label.to(device))\n",
        "            loss.backward()\n",
        "            train_loss += loss.item() * input.size(1)\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss = train_loss / len(dataloader.dataset)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "        #torch.save(BiLSTM_model.state_dict(),'BiLSTM_b1_epoch_' + str(i) + '.pt')\n",
        "        #BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
        "        BiLSTM_model.to(device)\n",
        "        print(label_dictionary)\n",
        "        revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "        revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "        devTemp = open(\"./devtemp.out\", 'w')\n",
        "        dev1 = open(\"./dev1.out\", 'w')\n",
        "        val_loss = 0.0\n",
        "        for devData, label, devDataLength, labelDataLength in dataloader_dev:\n",
        "\n",
        "            prediction = BiLSTM_model(devData.to(device), devDataLength)\n",
        "\n",
        "            output = prediction.view(-1, len(label_dictionary))\n",
        "            label2 = label.view(-1)\n",
        "            loss = criteria(output, label2.to(device))\n",
        "            loss.backward()\n",
        "            val_loss += loss.item() * devData.size(1)\n",
        "\n",
        "            prediction = prediction.cpu()\n",
        "            prediction = prediction.detach().numpy()\n",
        "            label = label.detach().numpy()\n",
        "            devData = devData.detach().numpy()\n",
        "            prediction = np.argmax(prediction, axis=2)\n",
        "            prediction = prediction.reshape((len(label), -1))\n",
        "\n",
        "            for i in range(len(devData)):\n",
        "                for j in range(len(devData[i])):\n",
        "                    if devData[i][j] != 0:\n",
        "                        word = revVocabDict[devData[i][j]]\n",
        "                        gold = revLabelDict[label[i][j]]\n",
        "                        op = revLabelDict[prediction[i][j]]\n",
        "                        devTemp.write(\" \".join([str(j+1), word, gold, op]))\n",
        "                        devTemp.write(\"\\n\")\n",
        "                        dev1.write(\" \".join([str(j+1), word, op]))\n",
        "                        dev1.write(\"\\n\")\n",
        "                devTemp.write(\"\\n\")        \n",
        "                dev1.write(\"\\n\")\n",
        "        devTemp.close()\n",
        "        dev1.close()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch == epochs:\n",
        "          print('model saved')\n",
        "          torch.save(BiLSTM_model.state_dict(),'blstm1.pt')\n",
        "\n",
        "        !perl conll03eval.txt < devtemp.out\n",
        "\n",
        "  def train_glove(self):\n",
        "\n",
        "    glove = pd.read_csv('./glove.6B.100d.gz', sep=\" \",\n",
        "                        quoting=3, header=None, index_col=0)\n",
        "    glove_emb = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "    word_idx = getVectors.prepareWordIndex(getVectors(), x_train, x_val)\n",
        "    glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
        "    glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
        "    glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
        "    emb_matrix = getVectors.createEmbedMatrix(getVectors(),\n",
        "        wordIdx=word_idx, embedDictionary=glove_emb, dimension=100)\n",
        "\n",
        "    vocab_size = emb_matrix.shape[0]\n",
        "    vector_size = emb_matrix.shape[1]\n",
        "    print(vocab_size, vector_size)\n",
        "    Glove_Model = BiLSTM_Glove(vocabSize=len(word_idx),\n",
        "                          embedDimension=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary),\n",
        "                          embedMatrix=emb_matrix)\n",
        "    Glove_Model.to(device)\n",
        "    print(Glove_Model)\n",
        "\n",
        "    BiLSTM_train = BiLSTM_DataLoader(x_train_vectors, y_train_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader = DataLoader(dataset=BiLSTM_train,\n",
        "                            batch_size=8,\n",
        "                            drop_last=True,\n",
        "                            collate_fn=custom_collator)\n",
        "\n",
        "    BiLSTM_dev = BiLSTM_DataLoader(x_val_vectors, y_val_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
        "                                batch_size=1,\n",
        "                                shuffle=False,\n",
        "                                drop_last=True,\n",
        "                                collate_fn=custom_collator)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
        "    criterion = criterion.to(device)\n",
        "    criterion.requres_grad = True\n",
        "    optimizer = torch.optim.SGD(Glove_Model.parameters(), lr=0.1, momentum=0.9)\n",
        "    scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
        "    epochs = 25\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = 0.0\n",
        "        for input, label, input_len, label_len in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = Glove_Model(input.to(device), input_len)\n",
        "            output = output.view(-1, len(label_dictionary))\n",
        "            label = label.view(-1)\n",
        "            loss = criterion(output, label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * input.size(1)\n",
        "\n",
        "        train_loss = train_loss / len(dataloader.dataset)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "\n",
        "        #torch.save(Glove_Model.state_dict(),'BiLSTM_glove_' + str(i) + '.pt')\n",
        "        #Glove_Model.load_state_dict(torch.load(\"./BiLSTM_glove_\"+str(e)+\".pt\"))#125\n",
        "\n",
        "        Glove_Model.to(device)\n",
        "        revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "        revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "        \n",
        "        devTemp2 = open(\"devtemp2.out\", 'w')\n",
        "        dev2 = open(\"dev2.out\", 'w')\n",
        "\n",
        "        for devData, label, devDataLength, label_data_len in dataloader_dev:\n",
        "\n",
        "            prediction = Glove_Model(devData.to(device), devDataLength)\n",
        "            prediction = prediction.cpu()\n",
        "            prediction = prediction.detach().numpy()\n",
        "            label = label.detach().numpy()\n",
        "            devData = devData.detach().numpy()\n",
        "            prediction = np.argmax(prediction, axis=2)\n",
        "            prediction = prediction.reshape((len(label), -1))\n",
        "\n",
        "            for i in range(len(devData)):\n",
        "                for j in range(len(devData[i])):\n",
        "                    if devData[i][j] != 0:\n",
        "                        word = revVocabDict[devData[i][j]]\n",
        "                        gold = revLabelDict[label[i][j]]\n",
        "                        op = revLabelDict[prediction[i][j]]\n",
        "                        devTemp2.write(\" \".join([str(j+1), word, gold, op]))\n",
        "                        devTemp2.write(\"\\n\")\n",
        "                        dev2.write(\" \".join([str(j+1), word, op]))\n",
        "                        dev2.write(\"\\n\")\n",
        "                devTemp2.write(\"\\n\")\n",
        "                dev2.write(\"\\n\")\n",
        "        devTemp2.close()\n",
        "        dev2.close()\n",
        "\n",
        "        if epoch == epochs:\n",
        "          print('model saved')\n",
        "          torch.save(Glove_Model.state_dict(),'blstm2.pt')\n",
        "\n",
        "        !perl conll03eval.txt < devtemp2.out\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_lstm()\n",
        "    self.train_glove()\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing: \n",
        "\n",
        "The class below is used for testing the data. It has 2 functions, t_lstm, which tests the Simple BiLSTM model, and test_glove which tests the Glove model using the testing dataset. \\\n",
        "The outputs are stored in files test1.out and test2.out."
      ],
      "metadata": {
        "id": "HBUlDapnyw1B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYB7FsUlAJry"
      },
      "outputs": [],
      "source": [
        "class test():\n",
        "  def test_lstm(self):\n",
        "    BiLSTM_test = BiLSTM_TestLoader(x_test_vectors)\n",
        "    custom_test_collator = CustomTestCollator(word_idx, label_dictionary)\n",
        "    dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    drop_last=True,\n",
        "                                    collate_fn=custom_test_collator)\n",
        "    \n",
        "    BiLSTM_model = BiLSTM(vocabSize=len(word_idx),\n",
        "                          embedDimensions=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary))\n",
        "\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
        "    BiLSTM_model.to(device)\n",
        "\n",
        "    revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "    revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    test1 = open(\"test1.out\", 'w')\n",
        "    for testData, testDataLength in dataloader_test:\n",
        "      prediction = BiLSTM_model(testData.to(device), testDataLength)\n",
        "      prediction = prediction.cpu()\n",
        "      prediction = prediction.detach().numpy()\n",
        "      testData = testData.detach().numpy()\n",
        "      prediction = np.argmax(prediction, axis=2)\n",
        "      prediction = prediction.reshape((len(testData), -1))\n",
        "            \n",
        "      for i in range(len(testData)):\n",
        "        for j in range(len(testData[i])):\n",
        "          if testData[i][j] != 0:\n",
        "            word = revVocabDict[testData[i][j]]\n",
        "            op = revLabelDict[prediction[i][j]]\n",
        "            test1.write(\" \".join([str(j+1), word, op]))\n",
        "            test1.write(\"\\n\")\n",
        "        test1.write(\"\\n\")        \n",
        "    test1.close()\n",
        "\n",
        "  def test_glove(self):\n",
        "    glove = pd.read_csv('./glove.6B.100d.gz', sep=\" \",\n",
        "                        quoting=3, header=None, index_col=0)\n",
        "    glove_emb = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "    word_idx = getVectors.prepareWordIndex(getVectors(), x_train, x_val)\n",
        "    glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
        "    glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
        "    glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
        "    emb_matrix = getVectors.createEmbedMatrix(getVectors(),\n",
        "        wordIdx=word_idx, embedDictionary=glove_emb, dimension=100)\n",
        "\n",
        "    BiLSTM_test = BiLSTM_TestLoader(x_test_vectors)\n",
        "    custom_test_collator = CustomTestCollator(word_idx, label_dictionary)\n",
        "    dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    drop_last=True,\n",
        "                                    collate_fn=custom_test_collator)\n",
        "    \n",
        "    Glove_model = BiLSTM_Glove(vocabSize=len(word_idx),\n",
        "                          embedDimension=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary),\n",
        "                          embedMatrix=emb_matrix)\n",
        "    \n",
        "    Glove_model.load_state_dict(torch.load(\"./blstm2.pt\"))#125\n",
        "    Glove_model.to(device)\n",
        "    revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "    revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    test2 = open(\"test2.out\", 'w')\n",
        "    for testData, testDataLength in dataloader_test:\n",
        "      prediction = Glove_model(testData.to(device), testDataLength)\n",
        "      prediction = prediction.cpu()\n",
        "      prediction = prediction.detach().numpy()\n",
        "      testData = testData.detach().numpy()\n",
        "      prediction = np.argmax(prediction, axis=2)\n",
        "      prediction = prediction.reshape((len(testData), -1))\n",
        "            \n",
        "      for i in range(len(testData)):\n",
        "        for j in range(len(testData[i])):\n",
        "          if testData[i][j] != 0:\n",
        "            word = revVocabDict[testData[i][j]]\n",
        "            op = revLabelDict[prediction[i][j]]\n",
        "            test2.write(\" \".join([str(j+1), word, op]))\n",
        "            test2.write(\"\\n\")\n",
        "        test2.write(\"\\n\")        \n",
        "    test2.close()\n",
        "\n",
        "testObj = test()\n",
        "testObj.test_glove()\n",
        "testObj.test_lstm() "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
        "import random\n",
        "import json\n",
        "from argparse import ArgumentParser\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\"\"\"Below is a class, createData.\"\"\"\n",
        "\n",
        "class createData():\n",
        "    \n",
        "  def SplitTrainXY(self, dataset):\n",
        "      x_train, y_train = list(), list()\n",
        "      x, y = list(), list()\n",
        "      first = 1\n",
        "      for entry in dataset.itertuples():\n",
        "          # print(type(entry.id))\n",
        "          # break\n",
        "          if(entry.ID == '1' and first == 0):\n",
        "              x_train.append(x)\n",
        "              y_train.append(y)\n",
        "              x = list()\n",
        "              y = list()\n",
        "          first = 0\n",
        "          x.append(entry.Word)\n",
        "          y.append(entry.NERTag)\n",
        "\n",
        "      x_train.append(x)\n",
        "      y_train.append(y)\n",
        "\n",
        "      return x_train, y_train\n",
        "\n",
        "  def readFileTrain(self, filepath):\n",
        "      train_df = list()\n",
        "      with open(filepath, 'r') as inputFile:\n",
        "          for line in inputFile.readlines():\n",
        "              if len(line) > 2:\n",
        "                id, word, ner_tag = line.strip().split(\" \")\n",
        "                train_df.append([id, word, ner_tag])\n",
        "\n",
        "      train_df = pd.DataFrame(train_df, columns=['ID', 'Word', 'NERTag'])\n",
        "      train_df = train_df.dropna()\n",
        "      x_train, y_train = self.SplitTrainXY(train_df)\n",
        "      return x_train, y_train\n",
        "\n",
        "  def SplitTestX(self, dataset):\n",
        "      x_train = list()\n",
        "      x = list()\n",
        "      first = 1\n",
        "      for entry in dataset.itertuples():\n",
        "          # print(type(entry.id))\n",
        "          # break\n",
        "          if(entry.ID == '1' and first == 0):\n",
        "              x_train.append(x)\n",
        "              x = list()\n",
        "          first = 0\n",
        "          x.append(entry.Word)\n",
        "\n",
        "      x_train.append(x)\n",
        "      return x_train\n",
        "\n",
        "  def readFileTest(self, filepath):\n",
        "      train_df = list()\n",
        "      with open(filepath, 'r') as inputFile:\n",
        "          for line in inputFile.readlines():\n",
        "              if len(line) > 1:\n",
        "                  id, word = line.strip().split(\" \")\n",
        "                  train_df.append([id, word])\n",
        "\n",
        "      train_df = pd.DataFrame(train_df, columns=['ID', 'Word'])\n",
        "      train_df = train_df.dropna()\n",
        "      x_train = self.SplitTestX(train_df)\n",
        "      return x_train\n",
        "\n",
        "\n",
        "\"\"\"Here we have a class getVectors() which is used to generate and return the various vectors and matrices required for training the BiLSTM models.\"\"\"\n",
        "\n",
        "class getVectors():\n",
        "\n",
        "  def prepareVocabulary(self, dataset):\n",
        "      vocab = set()\n",
        "      for sentence in dataset:\n",
        "          for word in sentence:\n",
        "              vocab.add(word)\n",
        "      return vocab\n",
        "  \n",
        "  def prepareLabelDictionary(self, train_y, val_y):\n",
        "      label1 = self.prepareVocabulary(train_y)\n",
        "      label2 = self.prepareVocabulary(val_y)\n",
        "      label = label1.union(label2)\n",
        "      label_tuples = []\n",
        "      counter = 0\n",
        "      for tags in label:\n",
        "          label_tuples.append((tags, counter))\n",
        "          counter += 1\n",
        "      label_dict = dict(label_tuples)\n",
        "      return label_dict\n",
        "\n",
        "  def prepareWordIndex(self, train, val):\n",
        "      wordIdx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "      idx = 2\n",
        "      for data in [train, val]:\n",
        "          for sent in data:\n",
        "              for word in sent:\n",
        "                  if word not in wordIdx:\n",
        "                      wordIdx[word] = idx\n",
        "                      idx += 1\n",
        "      return wordIdx\n",
        "\n",
        "  def vectorizeSentence(self, train, wordIdx):\n",
        "      train_vec = list()\n",
        "      tmp_x = list()\n",
        "      for words in train:\n",
        "          for word in words:\n",
        "            if word not in wordIdx.keys():\n",
        "              tmp_x.append(wordIdx['<UNK>'])\n",
        "            else:\n",
        "              tmp_x.append(wordIdx[word])\n",
        "          train_vec.append(tmp_x)\n",
        "          tmp_x = list()\n",
        "      return train_vec\n",
        "\n",
        "  def vectorizeLabel(self, train_y, label_dict):\n",
        "      train_y_vec = list()\n",
        "      for tags in train_y:\n",
        "          tmp_yy = list()\n",
        "          for label in tags:\n",
        "              tmp_yy.append(label_dict[label])\n",
        "          train_y_vec.append(tmp_yy)\n",
        "      return train_y_vec\n",
        "  \n",
        "  def createEmbedMatrix(self, wordIdx, embedDictionary, dimension):\n",
        "      embedMatrix = np.zeros((len(wordIdx), dimension))\n",
        "      for word, idx in wordIdx.items():\n",
        "          if word in embedDictionary:\n",
        "              embedMatrix[idx] = embedDictionary[word]\n",
        "          else:\n",
        "              if word.lower() in embedDictionary:\n",
        "                  embedMatrix[idx] = embedDictionary[word.lower()] + 5e-3\n",
        "              else:\n",
        "                  embedMatrix[idx] = embedDictionary[\"<UNK>\"]\n",
        "\n",
        "      return embedMatrix\n",
        "      \n",
        "  def initializeClassWeights(self, label_dict, train_y, val_y):\n",
        "    classWeights = dict()\n",
        "    for key in label_dict:\n",
        "        classWeights[key] = 0\n",
        "    total_nm_tags = 0\n",
        "    for data in [train_y, val_y]:\n",
        "        for tags in data:\n",
        "            for tag in tags:\n",
        "                total_nm_tags += 1\n",
        "                classWeights[tag] += 1\n",
        "\n",
        "    classWt = list()\n",
        "    for key in classWeights.keys():\n",
        "        if classWeights[key]:\n",
        "            score = round(math.log(0.35*total_nm_tags / classWeights[key]), 2)\n",
        "            classWeights[key] = score if score > 1.0 else 1.0\n",
        "        else:\n",
        "            classWeights[key] = 1.0\n",
        "        classWt.append(classWeights[key])\n",
        "    classWt = torch.tensor(classWt)\n",
        "    return classWt\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"## Task 1: Simple Bidirectional LSTM\n",
        "\n",
        "The class below creates a BiLSTM model with the following parameters and architecture:\n",
        "\n",
        "Architecture: Embedding layer --> LSTM --> Linear layer --> ELU --> Classifier\n",
        "\n",
        "Parameters: \\\n",
        "1. Embedding Dimension: 100\n",
        "2. Number of LSTM layers: 1\n",
        "3. LSTM Hidden Dimensions: 256\n",
        "4. LSTM Dropout: 0.33\n",
        "5. Linear Layer Output Dimensions: 128\n",
        "\"\"\"\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDimensions, linearOutputDimension, hiddenLayerDimension, LSTMLayers,\n",
        "                 bidirectional, dropoutValue, tagSize):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.hiddenLayerDimension = hiddenLayerDimension \n",
        "        self.LSTMLayers = LSTMLayers \n",
        "        self.embedDimensions = embedDimensions \n",
        "        self.linearOutputDimension = linearOutputDimension  \n",
        "        self.tagSize = tagSize \n",
        "        if bidirectional:\n",
        "          self.num_directions = 2 \n",
        "        else:\n",
        "          self.num_directions = 1\n",
        "\n",
        "        \"\"\" Initializing Network \"\"\"\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocabSize, embedDimensions) \n",
        "        self.embedding.weight.data.uniform_(-1,1)\n",
        "\n",
        "        #BiLSTM Layer\n",
        "        self.LSTM = nn.LSTM(embedDimensions,\n",
        "                            hiddenLayerDimension,\n",
        "                            num_layers=LSTMLayers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        #Linear Layer\n",
        "        self.fc = nn.Linear(hiddenLayerDimension*self.num_directions,\n",
        "                            linearOutputDimension)  \n",
        "        \n",
        "        #Dropout Layer\n",
        "        self.dropout = nn.Dropout(dropoutValue)\n",
        "\n",
        "        #ELU Layer\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "\n",
        "        #Classifier \n",
        "        self.classifier = nn.Linear(linearOutputDimension, self.tagSize)\n",
        "\n",
        "    def init_hidden(self, batchSize):\n",
        "        h, c = (torch.zeros(self.LSTMLayers * self.num_directions,\n",
        "                            batchSize, self.hiddenLayerDimension).to(device),\n",
        "                torch.zeros(self.LSTMLayers * self.num_directions,\n",
        "                            batchSize, self.hiddenLayerDimension).to(device))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, sentence, sentenceLength):  \n",
        "        # Set initial states\n",
        "        batchSize = sentence.shape[0]\n",
        "        h_0, c_0 = self.init_hidden(batchSize)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        embedded = self.embedding(sentence).float()\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded, sentenceLength, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
        "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        dropout = self.dropout(output_unpacked)\n",
        "        linear = self.fc(dropout)\n",
        "        prediction = self.elu(linear)\n",
        "        prediction = self.classifier(prediction)\n",
        "        return prediction\n",
        "\n",
        "\"\"\"## Task 2: Glove word Embeddings\n",
        "\n",
        "The class below creates a BiLSTM model with the glove vectors with the following parameters and architecture: \n",
        "\n",
        "Architecture: Embedding layer --> LSTM --> Linear layer --> ELU --> Classifier\n",
        "\n",
        "Parameters: \\\n",
        "1. Embedding Dimension: 100\n",
        "2. Number of LSTM layers: 1\n",
        "3. LSTM Hidden Dimensions: 256\n",
        "4. LSTM Dropout: 0.33\n",
        "5. Linear Layer Output Dimensions: 128\n",
        "\"\"\"\n",
        "\n",
        "class BiLSTM_Glove(nn.Module):\n",
        "    def __init__(self, vocabSize, embedDimension, linearOutputDimension, hiddenLayerDimension, LSTMLayers,\n",
        "                 bidirectional, dropoutValue, tagSize, embedMatrix):\n",
        "        super(BiLSTM_Glove, self).__init__()\n",
        "        \"\"\" Hyper Parameters \"\"\"\n",
        "        self.hiddenLayerDimension = hiddenLayerDimension \n",
        "        self.LSTMLayers = LSTMLayers \n",
        "        self.embedDimension = embedDimension \n",
        "        self.linearOutputDimension = linearOutputDimension \n",
        "        self.tagSize = tagSize \n",
        "        self.embedMatrix = embedMatrix\n",
        "        if bidirectional:\n",
        "          self.numDirections = 2 \n",
        "        else:\n",
        "          self.numDirections = 1\n",
        "\n",
        "        \"\"\" Initializing Network \"\"\"\n",
        "        #Embedding layer\n",
        "        self.embedding = nn.Embedding(vocabSize, embedDimension)  \n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedMatrix))\n",
        "\n",
        "        #LSTM Layer\n",
        "        self.LSTM = nn.LSTM(embedDimension,\n",
        "                            hiddenLayerDimension,\n",
        "                            num_layers=LSTMLayers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        #Linear layer\n",
        "        self.fc = nn.Linear(hiddenLayerDimension*self.numDirections, linearOutputDimension) \n",
        "\n",
        "        #Dropout layer\n",
        "        self.dropout = nn.Dropout(dropoutValue)\n",
        "\n",
        "        #ELU Layer\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "\n",
        "        #CLassifier\n",
        "        self.classifier = nn.Linear(linearOutputDimension, self.tagSize)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h, c = (torch.zeros(self.LSTMLayers * self.numDirections,\n",
        "                            batch_size, self.hiddenLayerDimension).to(device),\n",
        "                torch.zeros(self.LSTMLayers * self.numDirections,\n",
        "                            batch_size, self.hiddenLayerDimension).to(device))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, sentence, sentenceLength):  \n",
        "        #Set initial states\n",
        "        batch_size = sentence.shape[0]\n",
        "        h_0, c_0 = self.init_hidden(batch_size)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        embedded = self.embedding(sentence).float()\n",
        "        packed_embedded = pack_padded_sequence(embedded, sentenceLength, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
        "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        dropout = self.dropout(output_unpacked)\n",
        "        linear = self.fc(dropout)\n",
        "        prediction = self.elu(linear)\n",
        "        prediction = self.classifier(prediction)\n",
        "        return prediction\n",
        "\n",
        "\"\"\"The two classes below, BiLSTM_DataLoader and BiLSTM_TestLoader load the dataset by converting the data into torch tensors. \"\"\"\n",
        "\n",
        "class BiLSTM_DataLoader(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        xInstance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
        "        yInstance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
        "        return xInstance, yInstance\n",
        "\n",
        "class BiLSTM_TestLoader(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        xInstance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
        "        return xInstance\n",
        "\n",
        "\"\"\"The two classes below, CustomCollator and CustomTestCollator both implement padding for the input data by considering the max padding length as the max length in the Batch. \"\"\"\n",
        "\n",
        "class CustomCollator(object):\n",
        "\n",
        "    def __init__(self, vocabulary, label):\n",
        "        self.params = vocabulary\n",
        "        self.label = label\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        (xx, yy) = zip(*batch)\n",
        "        xLength = [len(x) for x in xx]\n",
        "        ylength = [len(y) for y in yy]\n",
        "        maxBatchlength = max([len(s) for s in xx])\n",
        "        batchData = self.params['<PAD>']*np.ones((len(xx), maxBatchlength))\n",
        "        batchLabels = -1*np.zeros((len(xx), maxBatchlength))\n",
        "        for j in range(len(xx)):\n",
        "            cur_len = len(xx[j])\n",
        "            batchData[j][:cur_len] = xx[j]\n",
        "            batchLabels[j][:cur_len] = yy[j]\n",
        "\n",
        "        batchData, batchLabels = torch.LongTensor(\n",
        "            batchData), torch.LongTensor(batchLabels)\n",
        "        batchData, batchLabels = Variable(batchData), Variable(batchLabels)\n",
        "\n",
        "        return batchData, batchLabels, xLength, ylength\n",
        "\n",
        "class CustomTestCollator(object):\n",
        "\n",
        "    def __init__(self, vocabulary, label):\n",
        "        self.params = vocabulary\n",
        "        self.label = label\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        xx = batch\n",
        "        xLen = [len(x) for x in xx]\n",
        "        maxBatchlength = max([len(s) for s in xx])\n",
        "        batchData = self.params['<PAD>']*np.ones((len(xx), maxBatchlength))\n",
        "        for j in range(len(xx)):\n",
        "            cur_len = len(xx[j])\n",
        "            batchData[j][:cur_len] = xx[j]\n",
        "\n",
        "        batchData = torch.LongTensor(batchData)\n",
        "        batchData = Variable(batchData)\n",
        "\n",
        "        return batchData, xLen\n",
        "\n",
        "\"\"\"## Training:\n",
        "\n",
        "The class below is used for training the data. It has 2 functions, train_lstm, which trains the Simple BiLSTM model, and train_glove which trains the Glove model using the training and dev datasets. \n",
        "\n",
        "1. Simple BiLSTM Training: \\\n",
        "It is trained using the following parameters: \\\n",
        "  *   batch size: 4\n",
        "  *   optimizer: SGD, with learning_rate = 0.1 and momentum = 0.9\n",
        "  *   scheduler: StepLR, with step_size = 20 and gamma = 0.9\n",
        "\n",
        "2. BiLSTM Training using Glove Embeddings: \\\n",
        "It is trained using the following parameters: \\\n",
        "  *   batch size: 8\n",
        "  *   optimizer: SGD, with learning_rate = 0.1 and momentum = 0.9\n",
        "  *   scheduler: StepLR, with step_size = 15 and gamma = 0.9\n",
        "\n",
        "The dev outputs are stored in 2 files, dev1.out and dev2.out.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class train():\n",
        "\n",
        "  def train_lstm(self):\n",
        "    BiLSTM_model = BiLSTM(vocabSize=len(word_idx),\n",
        "                          embedDimensions=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary))\n",
        "    BiLSTM_model.to(device)\n",
        "    print(BiLSTM_model)\n",
        "\n",
        "    BiLSTM_train = BiLSTM_DataLoader(x_train_vectors, y_train_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader = DataLoader(dataset=BiLSTM_train,\n",
        "                            batch_size=4,\n",
        "                            drop_last=True,\n",
        "                            collate_fn=custom_collator)\n",
        "\n",
        "    BiLSTM_dev = BiLSTM_DataLoader(x_val_vectors, y_val_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
        "                                batch_size=1,\n",
        "                                shuffle=False,\n",
        "                                drop_last=True,\n",
        "                                collate_fn=custom_collator)\n",
        "\n",
        "    criteria = nn.CrossEntropyLoss(weight=class_wt)\n",
        "    criteria = criteria.to(device)\n",
        "    criteria.requres_grad = True\n",
        "    optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
        "    scheduler = StepLR(optimizer, step_size = 20, gamma=0.9)\n",
        "    epochs = 50\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = 0.0\n",
        "        for input, label, inputLength, labelLength in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = BiLSTM_model(input.to(device), inputLength)\n",
        "            output = output.view(-1, len(label_dictionary))\n",
        "            label = label.view(-1)\n",
        "            loss = criteria(output, label.to(device))\n",
        "            loss.backward()\n",
        "            train_loss += loss.item() * input.size(1)\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss = train_loss / len(dataloader.dataset)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "        #torch.save(BiLSTM_model.state_dict(),'BiLSTM_b1_epoch_' + str(i) + '.pt')\n",
        "        #BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
        "        BiLSTM_model.to(device)\n",
        "        print(label_dictionary)\n",
        "        revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "        revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "        #devTemp = open(\"./devtemp.out\", 'w')\n",
        "        dev1 = open(\"./dev1.out\", 'w')\n",
        "        val_loss = 0.0\n",
        "        for devData, label, devDataLength, labelDataLength in dataloader_dev:\n",
        "\n",
        "            prediction = BiLSTM_model(devData.to(device), devDataLength)\n",
        "\n",
        "            output = prediction.view(-1, len(label_dictionary))\n",
        "            label2 = label.view(-1)\n",
        "            loss = criteria(output, label2.to(device))\n",
        "            loss.backward()\n",
        "            val_loss += loss.item() * devData.size(1)\n",
        "\n",
        "            prediction = prediction.cpu()\n",
        "            prediction = prediction.detach().numpy()\n",
        "            label = label.detach().numpy()\n",
        "            devData = devData.detach().numpy()\n",
        "            prediction = np.argmax(prediction, axis=2)\n",
        "            prediction = prediction.reshape((len(label), -1))\n",
        "\n",
        "            for i in range(len(devData)):\n",
        "                for j in range(len(devData[i])):\n",
        "                    if devData[i][j] != 0:\n",
        "                        word = revVocabDict[devData[i][j]]\n",
        "                        gold = revLabelDict[label[i][j]]\n",
        "                        op = revLabelDict[prediction[i][j]]\n",
        "                        #devTemp.write(\" \".join([str(j+1), word, gold, op]))\n",
        "                        #devTemp.write(\"\\n\")\n",
        "                        dev1.write(\" \".join([str(j+1), word, op]))\n",
        "                        dev1.write(\"\\n\")\n",
        "                #devTemp.write(\"\\n\")\n",
        "                dev1.write(\"\\n\")\n",
        "        #devTemp.close()\n",
        "        dev1.close()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch == epochs:\n",
        "          #print('model saved')\n",
        "          torch.save(BiLSTM_model.state_dict(),'blstm1.pt')\n",
        "\n",
        "        #!perl conll03eval.txt < devtemp.out\n",
        "\n",
        "  def train_glove(self):\n",
        "\n",
        "    glove = pd.read_csv('./glove.6B.100d.gz', sep=\" \",\n",
        "                        quoting=3, header=None, index_col=0)\n",
        "    glove_emb = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "    word_idx = getVectors.prepareWordIndex(getVectors(), x_train, x_val)\n",
        "    glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
        "    glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
        "    glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
        "    emb_matrix = getVectors.createEmbedMatrix(getVectors(),\n",
        "        wordIdx=word_idx, embedDictionary=glove_emb, dimension=100)\n",
        "\n",
        "    vocab_size = emb_matrix.shape[0]\n",
        "    vector_size = emb_matrix.shape[1]\n",
        "    print(vocab_size, vector_size)\n",
        "    Glove_Model = BiLSTM_Glove(vocabSize=len(word_idx),\n",
        "                          embedDimension=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary),\n",
        "                          embedMatrix=emb_matrix)\n",
        "    Glove_Model.to(device)\n",
        "    print(Glove_Model)\n",
        "\n",
        "    BiLSTM_train = BiLSTM_DataLoader(x_train_vectors, y_train_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader = DataLoader(dataset=BiLSTM_train,\n",
        "                            batch_size=8,\n",
        "                            drop_last=True,\n",
        "                            collate_fn=custom_collator)\n",
        "\n",
        "    BiLSTM_dev = BiLSTM_DataLoader(x_val_vectors, y_val_vectors)\n",
        "    custom_collator = CustomCollator(word_idx, label_dictionary)\n",
        "    dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
        "                                batch_size=1,\n",
        "                                shuffle=False,\n",
        "                                drop_last=True,\n",
        "                                collate_fn=custom_collator)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
        "    criterion = criterion.to(device)\n",
        "    criterion.requres_grad = True\n",
        "    optimizer = torch.optim.SGD(Glove_Model.parameters(), lr=0.1, momentum=0.9)\n",
        "    scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
        "    epochs = 25\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = 0.0\n",
        "        for input, label, input_len, label_len in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = Glove_Model(input.to(device), input_len)\n",
        "            output = output.view(-1, len(label_dictionary))\n",
        "            label = label.view(-1)\n",
        "            loss = criterion(output, label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * input.size(1)\n",
        "\n",
        "        train_loss = train_loss / len(dataloader.dataset)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "\n",
        "        #torch.save(Glove_Model.state_dict(),'BiLSTM_glove_' + str(i) + '.pt')\n",
        "        #Glove_Model.load_state_dict(torch.load(\"./BiLSTM_glove_\"+str(e)+\".pt\"))#125\n",
        "\n",
        "        Glove_Model.to(device)\n",
        "        revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "        revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "        \n",
        "        #devTemp2 = open(\"devtemp2.out\", 'w')\n",
        "        dev2 = open(\"dev2.out\", 'w')\n",
        "\n",
        "        for devData, label, devDataLength, label_data_len in dataloader_dev:\n",
        "\n",
        "            prediction = Glove_Model(devData.to(device), devDataLength)\n",
        "            prediction = prediction.cpu()\n",
        "            prediction = prediction.detach().numpy()\n",
        "            label = label.detach().numpy()\n",
        "            devData = devData.detach().numpy()\n",
        "            prediction = np.argmax(prediction, axis=2)\n",
        "            prediction = prediction.reshape((len(label), -1))\n",
        "\n",
        "            for i in range(len(devData)):\n",
        "                for j in range(len(devData[i])):\n",
        "                    if devData[i][j] != 0:\n",
        "                        word = revVocabDict[devData[i][j]]\n",
        "                        gold = revLabelDict[label[i][j]]\n",
        "                        op = revLabelDict[prediction[i][j]]\n",
        "                        #devTemp2.write(\" \".join([str(j+1), word, gold, op]))\n",
        "                        #devTemp2.write(\"\\n\")\n",
        "                        dev2.write(\" \".join([str(j+1), word, op]))\n",
        "                        dev2.write(\"\\n\")\n",
        "                #devTemp2.write(\"\\n\")\n",
        "                #dev2.write(\"\\n\")\n",
        "        #devTemp2.close()\n",
        "        dev2.close()\n",
        "\n",
        "        if epoch == epochs:\n",
        "          #print('model saved')\n",
        "          torch.save(Glove_Model.state_dict(),'blstm2.pt')\n",
        "\n",
        "        #!perl conll03eval.txt < devtemp2.out\n",
        "\n",
        "\"\"\"## Testing: \n",
        "\n",
        "The class below is used for testing the data. It has 2 functions, t_lstm, which tests the Simple BiLSTM model, and test_glove which tests the Glove model using the testing dataset.\n",
        "The outputs are stored in files test1.out and test2.out.\n",
        "\"\"\"\n",
        "\n",
        "class test():\n",
        "  def test_lstm(self):\n",
        "    BiLSTM_test = BiLSTM_TestLoader(x_test_vectors)\n",
        "    custom_test_collator = CustomTestCollator(word_idx, label_dictionary)\n",
        "    dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    drop_last=True,\n",
        "                                    collate_fn=custom_test_collator)\n",
        "    \n",
        "    BiLSTM_model = BiLSTM(vocabSize=len(word_idx),\n",
        "                          embedDimensions=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary))\n",
        "\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
        "    BiLSTM_model.to(device)\n",
        "\n",
        "    revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "    revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    test1 = open(\"test1.out\", 'w')\n",
        "    for testData, testDataLength in dataloader_test:\n",
        "      prediction = BiLSTM_model(testData.to(device), testDataLength)\n",
        "      prediction = prediction.cpu()\n",
        "      prediction = prediction.detach().numpy()\n",
        "      testData = testData.detach().numpy()\n",
        "      prediction = np.argmax(prediction, axis=2)\n",
        "      prediction = prediction.reshape((len(testData), -1))\n",
        "            \n",
        "      for i in range(len(testData)):\n",
        "        for j in range(len(testData[i])):\n",
        "          if testData[i][j] != 0:\n",
        "            word = revVocabDict[testData[i][j]]\n",
        "            op = revLabelDict[prediction[i][j]]\n",
        "            test1.write(\" \".join([str(j+1), word, op]))\n",
        "            test1.write(\"\\n\")\n",
        "        test1.write(\"\\n\")        \n",
        "    test1.close()\n",
        "\n",
        "  def test_glove(self):\n",
        "    glove = pd.read_csv('./glove.6B.100d.gz', sep=\" \",\n",
        "                        quoting=3, header=None, index_col=0)\n",
        "    glove_emb = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "    word_idx = getVectors.prepareWordIndex(getVectors(), x_train, x_val)\n",
        "    glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
        "    glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
        "    glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
        "    emb_matrix = getVectors.createEmbedMatrix(getVectors(),\n",
        "        wordIdx=word_idx, embedDictionary=glove_emb, dimension=100)\n",
        "\n",
        "    BiLSTM_test = BiLSTM_TestLoader(x_test_vectors)\n",
        "    custom_test_collator = CustomTestCollator(word_idx, label_dictionary)\n",
        "    dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    drop_last=True,\n",
        "                                    collate_fn=custom_test_collator)\n",
        "    \n",
        "    Glove_model = BiLSTM_Glove(vocabSize=len(word_idx),\n",
        "                          embedDimension=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary),\n",
        "                          embedMatrix=emb_matrix)\n",
        "    \n",
        "    Glove_model.load_state_dict(torch.load(\"./blstm2.pt\"))#125\n",
        "    Glove_model.to(device)\n",
        "    revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "    revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    test2 = open(\"test2.out\", 'w')\n",
        "    for testData, testDataLength in dataloader_test:\n",
        "      prediction = Glove_model(testData.to(device), testDataLength)\n",
        "      prediction = prediction.cpu()\n",
        "      prediction = prediction.detach().numpy()\n",
        "      testData = testData.detach().numpy()\n",
        "      prediction = np.argmax(prediction, axis=2)\n",
        "      prediction = prediction.reshape((len(testData), -1))\n",
        "            \n",
        "      for i in range(len(testData)):\n",
        "        for j in range(len(testData[i])):\n",
        "          if testData[i][j] != 0:\n",
        "            word = revVocabDict[testData[i][j]]\n",
        "            op = revLabelDict[prediction[i][j]]\n",
        "            test2.write(\" \".join([str(j+1), word, op]))\n",
        "            test2.write(\"\\n\")\n",
        "        test2.write(\"\\n\")        \n",
        "    test2.close()\n",
        "\n",
        "  def dev_lstm(self):\n",
        "    BiLSTM_test = BiLSTM_TestLoader(x_val_vectors)\n",
        "    custom_test_collator = CustomTestCollator(word_idx, label_dictionary)\n",
        "    dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    drop_last=True,\n",
        "                                    collate_fn=custom_test_collator)\n",
        "    \n",
        "    BiLSTM_model = BiLSTM(vocabSize=len(word_idx),\n",
        "                          embedDimensions=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary))\n",
        "\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
        "    BiLSTM_model.to(device)\n",
        "\n",
        "    revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "    revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    dev1 = open(\"dev1.out\", 'w')\n",
        "    for testData, testDataLength in dataloader_test:\n",
        "      prediction = BiLSTM_model(testData.to(device), testDataLength)\n",
        "      prediction = prediction.cpu()\n",
        "      prediction = prediction.detach().numpy()\n",
        "      testData = testData.detach().numpy()\n",
        "      prediction = np.argmax(prediction, axis=2)\n",
        "      prediction = prediction.reshape((len(testData), -1))\n",
        "            \n",
        "      for i in range(len(testData)):\n",
        "        for j in range(len(testData[i])):\n",
        "          if testData[i][j] != 0:\n",
        "            word = revVocabDict[testData[i][j]]\n",
        "            op = revLabelDict[prediction[i][j]]\n",
        "            dev1.write(\" \".join([str(j+1), word, op]))\n",
        "            dev1.write(\"\\n\")\n",
        "        dev1.write(\"\\n\")        \n",
        "    dev1.close()\n",
        "\n",
        "  def dev_glove(self):\n",
        "    glove = pd.read_csv('./glove.6B.100d.gz', sep=\" \",\n",
        "                        quoting=3, header=None, index_col=0)\n",
        "    glove_emb = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "    word_idx = getVectors.prepareWordIndex(getVectors(), x_train, x_val)\n",
        "    glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
        "    glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
        "    glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
        "    emb_matrix = getVectors.createEmbedMatrix(getVectors(),\n",
        "        wordIdx=word_idx, embedDictionary=glove_emb, dimension=100)\n",
        "\n",
        "    BiLSTM_test = BiLSTM_TestLoader(x_val_vectors)\n",
        "    custom_test_collator = CustomTestCollator(word_idx, label_dictionary)\n",
        "    dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    drop_last=True,\n",
        "                                    collate_fn=custom_test_collator)\n",
        "    \n",
        "    Glove_model = BiLSTM_Glove(vocabSize=len(word_idx),\n",
        "                          embedDimension=100,\n",
        "                          linearOutputDimension=128,\n",
        "                          hiddenLayerDimension=256,\n",
        "                          LSTMLayers=1,\n",
        "                          bidirectional=True,\n",
        "                          dropoutValue=0.33,\n",
        "                          tagSize=len(label_dictionary),\n",
        "                          embedMatrix=emb_matrix)\n",
        "    \n",
        "    Glove_model.load_state_dict(torch.load(\"./blstm2.pt\"))#125\n",
        "    Glove_model.to(device)\n",
        "    revLabelDict = {v: k for k, v in label_dictionary.items()}\n",
        "    revVocabDict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    dev2 = open(\"dev2.out\", 'w')\n",
        "    for testData, testDataLength in dataloader_test:\n",
        "      prediction = Glove_model(testData.to(device), testDataLength)\n",
        "      prediction = prediction.cpu()\n",
        "      prediction = prediction.detach().numpy()\n",
        "      testData = testData.detach().numpy()\n",
        "      prediction = np.argmax(prediction, axis=2)\n",
        "      prediction = prediction.reshape((len(testData), -1))\n",
        "            \n",
        "      for i in range(len(testData)):\n",
        "        for j in range(len(testData[i])):\n",
        "          if testData[i][j] != 0:\n",
        "            word = revVocabDict[testData[i][j]]\n",
        "            op = revLabelDict[prediction[i][j]]\n",
        "            dev2.write(\" \".join([str(j+1), word, op]))\n",
        "            dev2.write(\"\\n\")\n",
        "        dev2.write(\"\\n\")        \n",
        "    dev2.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    createObj = createData()\n",
        "    x_train, y_train = createObj.readFileTrain('./data/train')\n",
        "    x_val, y_val = createObj.readFileTrain('./data/dev')\n",
        "    x_test = createObj.readFileTest('./data/test')\n",
        "\n",
        "    vectorObj = getVectors()\n",
        "    word_idx = vectorObj.prepareWordIndex(x_train, x_val)\n",
        "    x_train_vectors = vectorObj.vectorizeSentence(x_train, word_idx)\n",
        "    x_test_vectors = vectorObj.vectorizeSentence(x_test, word_idx)\n",
        "    x_val_vectors = vectorObj.vectorizeSentence(x_val, word_idx)\n",
        "    label_dictionary = vectorObj.prepareLabelDictionary(y_train, y_val)\n",
        "    y_train_vectors = vectorObj.vectorizeLabel(y_train, label_dictionary)\n",
        "    y_val_vectors = vectorObj.vectorizeLabel(y_val, label_dictionary)\n",
        "\n",
        "    class_wt = vectorObj.initializeClassWeights(label_dictionary, y_train, y_val)\n",
        "    \n",
        "    \n",
        "    #parser = ArgumentParser()\n",
        "    #parser.add_argument(\"--mode\", \"-m\", type=str, choices=['train', 'test'], default='test')\n",
        "    args = 'test'\n",
        "    \n",
        "    if args == 'train':\n",
        "        trainObj = train()\n",
        "        trainObj.train_lstm()\n",
        "        trainObj.train_glove()\n",
        "    \n",
        "    elif args == 'test':\n",
        "        testObj = test()\n",
        "        testObj.test_glove()\n",
        "        testObj.test_lstm()\n",
        "        testObj.dev_lstm()\n",
        "        testObj.dev_glove()\n"
      ],
      "metadata": {
        "id": "Nl73m2Rh3PAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmMCC-Lt72bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}